{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8606605,"sourceType":"datasetVersion","datasetId":5149991},{"sourceId":8811221,"sourceType":"datasetVersion","datasetId":5299976},{"sourceId":8824840,"sourceType":"datasetVersion","datasetId":5309300}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Analysis of Taxi Trip Data\n\nThis notebook performs data analysis on taxi trip data, including data loading, transformation, and visualization. The steps include installing necessary packages, setting up display settings, connecting to the database, creating views for data analysis, and visualizing results.","metadata":{}},{"cell_type":"markdown","source":"\n\n##  Installation and Imports\n\n Install necessary packages and import libraries required for data manipulation and visualization.\n","metadata":{}},{"cell_type":"code","source":"# Install DuckDB Libraries\n!pip install -q duckdb","metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:45:38.676553Z","iopub.execute_input":"2024-07-03T05:45:38.677083Z","iopub.status.idle":"2024-07-03T05:45:56.685849Z","shell.execute_reply.started":"2024-07-03T05:45:38.677042Z","shell.execute_reply":"2024-07-03T05:45:56.684146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport duckdb as db\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.io as pio\nfrom IPython.core.interactiveshell import InteractiveShell\nfrom datetime import datetime, timedelta\nimport statsmodels.api as sm\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-03T05:45:58.750633Z","iopub.execute_input":"2024-07-03T05:45:58.751268Z","iopub.status.idle":"2024-07-03T05:46:02.473296Z","shell.execute_reply.started":"2024-07-03T05:45:58.751168Z","shell.execute_reply":"2024-07-03T05:46:02.472071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Display , Output and System Settings\nConfigure display settings for Pandas to ensure comprehensive visibility of data fields.","metadata":{}},{"cell_type":"code","source":"# Change Pandas display settings to see all fields.\n\n\npd.set_option('display.max_rows', 200)\npd.set_option('display.max_colwidth', 400)\npd.set_option('display.max_columns', 100)\npd.set_option(\"display.precision\", 5)\npd.set_option('display.width', 1000)\n\n\n#-----------------------------------\n# Modify the behavior of IPython/Jupyter notebook so that all expressions in a cell produce output, not just the last one.\n# This can make it easier to follow and debug code by displaying intermediate results.\n\n\nInteractiveShell.ast_node_interactivity = \"all\"\n\n#----------------------------------\n# Connect the database to memory for temporary data storing.\n\nconn = db.connect(\n    ':memory:'\n)\n\n# Modify database settings: memory limit/ see progress bar after running query/\n# enable output explanation for all queries/ set timezone to New York. \n\nconn.sql('''\nSET memory_limit = '25GB';\nSET enable_progress_bar = true;\nSET explain_output = 'all';\nSET TimeZone='America/New_York';\n'''\n)\n\n# See current database settings for parameters below:\n\nconn.sql('''\nSELECT *\nFROM duckdb_settings()\nWHERE name in ('TimeZone', 'Calendar', 'memory_limit', 'enable_progress_bar', 'explain_output', 'lock_configuration');\n'''\n)\n\n# Print current time of the selected timezone\n\nconn.sql('''\nSELECT now()\n'''\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:46:13.356655Z","iopub.execute_input":"2024-07-03T05:46:13.357242Z","iopub.status.idle":"2024-07-03T05:46:13.387344Z","shell.execute_reply.started":"2024-07-03T05:46:13.357208Z","shell.execute_reply":"2024-07-03T05:46:13.385922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Initial View\nDefine a view (tlctrip) combining trip data with location details for analysis.\n\n","metadata":{}},{"cell_type":"code","source":"# Create a view of our dataset based on our Dictionary.\n\n\nconn.sql('''\nCREATE OR REPLACE VIEW init_tlctrip AS\nSELECT \n    *,\n    CASE \n        WHEN cap_type = 'green' THEN lpep_pickup_datetime\n        WHEN cap_type = 'yellow' THEN tpep_pickup_datetime\n        ELSE NULL\n    END AS unified_pickup_datetime,\n    CASE \n        WHEN cap_type = 'green' THEN lpep_dropoff_datetime\n        WHEN cap_type = 'yellow' THEN tpep_dropoff_datetime\n        ELSE NULL\n    END AS unified_dropoff_datetime\nFROM \n    read_parquet('/kaggle/input/tlc-trip-2020-01-to-2024-03/trips/*/*/*.parquet', hive_partitioning = true)\nWHERE\n    (\n        unified_pickup_datetime < '2024-04-01' AND \n        unified_pickup_datetime >= '2020-01-01'\n    ) AND\n    (\n        (fare_amount >= 0) AND\n        (total_amount >= 0) AND\n        ((extra >= 0) OR (extra IS NULL)) AND\n        ((mta_tax >= 0) OR (mta_tax IS NULL)) AND\n        ((tip_amount >= 0) OR (tip_amount IS NULL)) AND\n        ((tolls_amount >= 0) OR (tolls_amount IS NULL)) AND\n        ((ehail_fee >= 0) OR (ehail_fee IS NULL)) AND\n        ((improvement_surcharge >= 0) OR (improvement_surcharge IS NULL)) AND\n        ((congestion_surcharge >= 0) OR (congestion_surcharge IS NULL)) AND\n        ((airport_fee >= 0) OR (airport_fee IS NULL))\n    )\n''')\n\n# Create a view of NewYork init_taxizone dataset.\n\nconn.sql('''\nCREATE OR REPLACE VIEW init_taxizone AS \nSELECT * FROM 'https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv'\n''')\n\n# Take a look on our init_tlctrip view.\n\nconn.sql('''\nSELECT * FROM init_tlctrip LIMIT 2\n''')\n\n# Take a look on our init_taxizone view.\n\nconn.sql('''\nSELECT * FROM init_taxizone LIMIT 2\n''')\n\n# Get a look on fields data types.\n\nconn.sql('''\nDESCRIBE init_tlctrip\n''').df()\n\nconn.sql('''\nDESCRIBE init_taxizone\n''').df()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:46:28.642944Z","iopub.execute_input":"2024-07-03T05:46:28.643363Z","iopub.status.idle":"2024-07-03T05:46:29.501764Z","shell.execute_reply.started":"2024-07-03T05:46:28.64333Z","shell.execute_reply":"2024-07-03T05:46:29.500634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Q.1**\n   * **1.1: Exploratory Analysis**\n        * 1.1.1: Total Count\n        * 1.1.2: Summary Statistics & Missing Values\n        * 1.1.3: Detect fields timeline\n        * 1.1.4: Validate Timestamps\n        * 1.1.5: Cap Ratio\n        * 1.1.6: Check Numeric Fields Values | Identify Outliers\n        * 1.1.7: Assess Categorical Value Consistency\n\n   * **1.2: Data Cleaning**\n        * 1.2.1: Identify Duplicate Records\n        * 1.2.2: Create Filtered View\n        * 1.2.3: Exclude Duplicates\n        * 1.2.4: Save Filtered_tlcTrip to a Parquet File\n       \n   * **1.3: Visualization**","metadata":{}},{"cell_type":"markdown","source":"## 1.1: Exploratory Analysis\n","metadata":{}},{"cell_type":"markdown","source":"###  1.1.1: Total Count","metadata":{}},{"cell_type":"code","source":"# Take a look on the dataset size.\n\nconn.sql('''\nSELECT \n    COUNT(*)\nFROM \n    init_tlctrip\n''').to_df()\n# Check init_taxizone count of records.\n\nconn.sql('''\nSELECT COUNT(*) FROM init_taxizone\n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:46:38.412084Z","iopub.execute_input":"2024-07-03T05:46:38.412667Z","iopub.status.idle":"2024-07-03T05:47:15.458197Z","shell.execute_reply.started":"2024-07-03T05:46:38.412626Z","shell.execute_reply":"2024-07-03T05:47:15.456948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.2: Summary Statistics & Missing Values","metadata":{}},{"cell_type":"code","source":"# Get a look on fields statistical summary.\n\nconn.sql('''\nSUMMARIZE init_tlctrip\n''').df()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:32:46.35271Z","iopub.execute_input":"2024-07-02T20:32:46.353145Z","iopub.status.idle":"2024-07-02T20:37:04.225622Z","shell.execute_reply.started":"2024-07-02T20:32:46.353111Z","shell.execute_reply":"2024-07-02T20:37:04.224394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.3: Detect fields timeline","metadata":{}},{"cell_type":"code","source":"# Check if all fields have data from 01-01-2020 to 31-03-2024.\n\nconn.sql('''\nSELECT min(unified_pickup_datetime) AS min_flag_datetime, max(unified_pickup_datetime) AS max_flag_datetime FROM init_tlctrip WHERE store_and_fwd_flag IS NOT NULL;\n''')\nconn.sql('''\nSELECT min(unified_pickup_datetime) AS min_fare_datetime, max(unified_pickup_datetime) AS max_distance_datetime FROM init_tlctrip WHERE RatecodeID IS NOT NULL;\n''')\nconn.sql('''\nSELECT min(unified_pickup_datetime) AS min_RatecodeID_datetime, max(unified_pickup_datetime) AS max_RatecodeID_datetime FROM init_tlctrip WHERE passenger_count IS NOT NULL;\n''')\nconn.sql('''\nSELECT min(unified_pickup_datetime) AS min_congestion_datetime, max(unified_pickup_datetime) AS max_congestion_datetime FROM init_tlctrip WHERE congestion_surcharge IS NOT NULL;\n''')\nconn.sql('''\nSELECT min(unified_pickup_datetime) AS min_airport_datetime, max(unified_pickup_datetime) AS max_airport_datetime FROM init_tlctrip WHERE airport_fee IS NOT NULL;\n''')\nconn.sql('''\nSELECT min(unified_pickup_datetime) AS min_payment_datetime, max(unified_pickup_datetime) AS max_payment_datetime FROM init_tlctrip WHERE payment_type IS NOT NULL;\n''')\nconn.sql('''\nSELECT min(unified_pickup_datetime) AS min_trip_datetime, max(unified_pickup_datetime) AS max_trip_datetime FROM init_tlctrip WHERE trip_type IS NOT NULL;\n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-03T06:15:19.732104Z","iopub.execute_input":"2024-07-03T06:15:19.732551Z","iopub.status.idle":"2024-07-03T06:17:43.459481Z","shell.execute_reply.started":"2024-07-03T06:15:19.732518Z","shell.execute_reply":"2024-07-03T06:17:43.458195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.4: Validate Timestamps\n","metadata":{}},{"cell_type":"code","source":"# Check if there are any records with wrong timestamp\n\nconn.sql('''\n    SELECT \n        COUNT(*) AS invalid_timestamps\n    FROM \n        init_tlctrip\n    WHERE \n        unified_pickup_datetime::TIMESTAMPTZ >= unified_dropoff_datetime::TIMESTAMPTZ\n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-03T06:18:56.571609Z","iopub.execute_input":"2024-07-03T06:18:56.572007Z","iopub.status.idle":"2024-07-03T06:20:20.750666Z","shell.execute_reply.started":"2024-07-03T06:18:56.571977Z","shell.execute_reply":"2024-07-03T06:20:20.749479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.5: Cap Ratio","metadata":{}},{"cell_type":"code","source":"# Check each 'cap_type' ratio to total.\n\nconn.sql('''\n    SELECT \n        cap_type, COUNT(*) AS cap_type_row, ROUND(cap_type_row*100/(SELECT COUNT(*) AS total_count FROM init_tlctrip), 2) AS percentage\n    FROM \n        init_tlctrip\n    GROUP BY\n        1\n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-03T06:21:23.372366Z","iopub.execute_input":"2024-07-03T06:21:23.372811Z","iopub.status.idle":"2024-07-03T06:22:05.602589Z","shell.execute_reply.started":"2024-07-03T06:21:23.372778Z","shell.execute_reply":"2024-07-03T06:22:05.601323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.6: Check Numeric Fields Values | Identify Outliers","metadata":{}},{"cell_type":"code","source":"# Check passenger_count values distribution.\n\nconn.sql('''\n    WITH passenger_bins AS (\n        SELECT \n            FLOOR(passenger_count / 7) * 7 AS passenger_bin,\n            COUNT(*) AS bin_count\n        FROM \n            init_tlctrip\n        WHERE \n            passenger_count IS NOT NULL\n        GROUP BY \n            FLOOR(passenger_count / 7) * 7\n    ),\n    total_count AS (\n        SELECT \n            COUNT(*) AS total\n        FROM \n            init_tlctrip\n        WHERE \n            passenger_count IS NOT NULL\n    ),\n    cumulative_bins AS (\n        SELECT \n            pb.passenger_bin,\n            pb.bin_count,\n            SUM(pb.bin_count) OVER (ORDER BY pb.passenger_bin) AS cumulative_count\n        FROM \n            passenger_bins pb\n    )\n    SELECT \n        CONCAT('0-', passenger_bin + 6) AS passenger_bin_range,\n        bin_count,\n        ROUND(cumulative_count * 100.0 / tc.total, 4) AS cumulative_percentage\n    FROM \n        cumulative_bins cb, total_count tc\n    ORDER BY \n        passenger_bin;\n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-03T06:22:56.931377Z","iopub.execute_input":"2024-07-03T06:22:56.931922Z","iopub.status.idle":"2024-07-03T06:23:41.888726Z","shell.execute_reply.started":"2024-07-03T06:22:56.931878Z","shell.execute_reply":"2024-07-03T06:23:41.887491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check trip_distance values distribution.\n\nconn.sql('''\n    WITH distance_bins AS (\n        SELECT \n            FLOOR(trip_distance / 0.07) * 0.07 AS distance_bin,\n            COUNT(*) AS bin_count\n        FROM \n            init_tlctrip\n        WHERE \n            trip_distance IS NOT NULL\n            AND trip_distance > 0\n        GROUP BY \n            FLOOR(trip_distance / 0.07) * 0.07\n    ),\n    total_count AS (\n        SELECT \n            COUNT(*) AS total\n        FROM \n            init_tlctrip\n        WHERE \n            trip_distance IS NOT NULL\n            AND trip_distance > 0\n    ),\n    cumulative_bins AS (\n        SELECT \n            db.distance_bin,\n            db.bin_count,\n            SUM(db.bin_count) OVER (ORDER BY db.distance_bin) AS cumulative_count\n        FROM \n            distance_bins db\n    )\n    SELECT \n        CONCAT('0-', distance_bin + 0.06) AS distance_bin_range,\n        bin_count,\n        ROUND(cumulative_count * 100.0 / tc.total, 4) AS cumulative_percentage\n    FROM \n        cumulative_bins cb, total_count tc\n    ORDER BY \n        distance_bin;\n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:42:08.6027Z","iopub.execute_input":"2024-07-02T20:42:08.603051Z","iopub.status.idle":"2024-07-02T20:42:55.001711Z","shell.execute_reply.started":"2024-07-02T20:42:08.603021Z","shell.execute_reply":"2024-07-02T20:42:55.000801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check speed values distribution.\n\nconn.sql('''\nWITH speed_bins AS (\n    SELECT \n        ROUND(trip_distance / (EXTRACT(EPOCH FROM unified_dropoff_datetime) - EXTRACT(EPOCH FROM unified_pickup_datetime)) * 3600 / 22.5) * 22.5 AS speed_bin,\n        COUNT(*) AS bin_count\n    FROM \n        init_tlctrip\n    WHERE \n        trip_distance > 0\n        AND (EXTRACT(EPOCH FROM unified_dropoff_datetime) - EXTRACT(EPOCH FROM unified_pickup_datetime)) > 0\n    GROUP BY \n        ROUND(trip_distance / (EXTRACT(EPOCH FROM unified_dropoff_datetime) - EXTRACT(EPOCH FROM unified_pickup_datetime)) * 3600 / 22.5) * 22.5\n),\ntotal_count AS (\n    SELECT \n        COUNT(*) AS total\n    FROM \n        init_tlctrip\n    WHERE \n        trip_distance > 0\n        AND (EXTRACT(EPOCH FROM unified_dropoff_datetime) - EXTRACT(EPOCH FROM unified_pickup_datetime)) > 0\n),\ncumulative_bins AS (\n    SELECT \n        sb.speed_bin,\n        sb.bin_count,\n        SUM(sb.bin_count) OVER (ORDER BY sb.speed_bin) AS cumulative_count\n    FROM \n        speed_bins sb\n)\nSELECT \n    CONCAT('0-', speed_bin + 22.5) AS speed_bin_range,\n    bin_count,\n    ROUND(cumulative_count * 100.0 / tc.total, 4) AS cumulative_percentage\nFROM \n    cumulative_bins cb, total_count tc\nORDER BY \n    speed_bin;\n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:42:55.003274Z","iopub.execute_input":"2024-07-02T20:42:55.004064Z","iopub.status.idle":"2024-07-02T20:43:52.415271Z","shell.execute_reply.started":"2024-07-02T20:42:55.004027Z","shell.execute_reply":"2024-07-02T20:43:52.414108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check trip_duration values distribution.\n\nconn.sql('''\n    WITH trip_durations AS (\n        SELECT\n            (EXTRACT(EPOCH FROM unified_dropoff_datetime) - EXTRACT(EPOCH FROM unified_pickup_datetime)) / 60 AS trip_duration_minutes\n        FROM\n            init_tlctrip\n        WHERE\n            unified_dropoff_datetime IS NOT NULL\n            AND unified_pickup_datetime IS NOT NULL\n            AND unified_pickup_datetime < unified_dropoff_datetime\n    ),\n    duration_bins AS (\n        SELECT\n            FLOOR(trip_duration_minutes / 30) * 30 AS duration_bin,\n            COUNT(*) AS bin_count\n        FROM\n            trip_durations\n        GROUP BY\n            FLOOR(trip_duration_minutes / 30) * 30\n    ),\n    total_count AS (\n        SELECT\n            COUNT(*) AS total\n        FROM\n            trip_durations\n    ),\n    cumulative_bins AS (\n        SELECT\n            db.duration_bin,\n            db.bin_count,\n            SUM(db.bin_count) OVER (ORDER BY db.duration_bin) AS cumulative_count\n        FROM\n            duration_bins db\n    )\n    SELECT\n        CONCAT('0-', duration_bin + 29) AS tripduration_bin_range,\n        bin_count,\n        ROUND(cumulative_count * 100.0 / tc.total, 4) AS cumulative_percentage\n    FROM\n        cumulative_bins cb, total_count tc\n    ORDER BY\n        duration_bin\n    LIMIT 10;\n''')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:43:52.42096Z","iopub.execute_input":"2024-07-02T20:43:52.421326Z","iopub.status.idle":"2024-07-02T20:44:43.946715Z","shell.execute_reply.started":"2024-07-02T20:43:52.421296Z","shell.execute_reply":"2024-07-02T20:44:43.945312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check fare_amount values distribution.\n\nconn.sql('''\n    WITH fare_bins AS (\n    SELECT \n        FLOOR(fare_amount / 100) * 100 AS fare_bin,\n        COUNT(*) AS bin_count\n    FROM \n        init_tlctrip\n    WHERE \n        fare_amount IS NOT NULL\n    GROUP BY \n        FLOOR(fare_amount / 100) * 100\n),\ntotal_count AS (\n    SELECT \n        COUNT(*) AS total\n    FROM \n        init_tlctrip\n    WHERE \n        fare_amount IS NOT NULL\n),\ncumulative_bins AS (\n    SELECT \n        fb.fare_bin,\n        fb.bin_count,\n        SUM(fb.bin_count) OVER (ORDER BY fb.fare_bin) AS cumulative_count\n    FROM \n        fare_bins fb\n)\nSELECT \n    CONCAT('0-', fare_bin + 99) AS fare_bin_range,\n    bin_count,\n    ROUND(cumulative_count * 100.0 / tc.total, 4) AS cumulative_percentage\nFROM \n    cumulative_bins cb, total_count tc\nORDER BY \n    fare_bin;\n\n''')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:44:43.948297Z","iopub.execute_input":"2024-07-02T20:44:43.94877Z","iopub.status.idle":"2024-07-02T20:45:26.918704Z","shell.execute_reply.started":"2024-07-02T20:44:43.94873Z","shell.execute_reply":"2024-07-02T20:45:26.917377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check extra values distribution.\n\nconn.sql('''\n    WITH extra_bins AS (\n        SELECT \n            FLOOR(extra / 10) * 10 AS extra_bin,\n            COUNT(*) AS bin_count\n        FROM \n            init_tlctrip\n        WHERE \n            extra IS NOT NULL\n        GROUP BY \n            FLOOR(extra / 10) * 10\n    ),\n    total_count AS (\n        SELECT \n            COUNT(*) AS total\n        FROM \n            init_tlctrip\n        WHERE \n            extra IS NOT NULL\n    ),\n    cumulative_bins AS (\n        SELECT \n            eb.extra_bin,\n            eb.bin_count,\n            SUM(eb.bin_count) OVER (ORDER BY eb.extra_bin) AS cumulative_count\n        FROM \n            extra_bins eb\n    )\n    SELECT \n        CONCAT('0-', extra_bin + 9) AS extra_bin_range,\n        bin_count,\n        ROUND(cumulative_count * 100.0 / tc.total, 4) AS cumulative_percentage\n    FROM \n        cumulative_bins cb, total_count tc\n    ORDER BY \n        extra_bin;\n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:45:26.920197Z","iopub.execute_input":"2024-07-02T20:45:26.920598Z","iopub.status.idle":"2024-07-02T20:46:10.034451Z","shell.execute_reply.started":"2024-07-02T20:45:26.920566Z","shell.execute_reply":"2024-07-02T20:46:10.033348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check mta_tax values distribution.\n\nconn.sql('''\n    WITH mta_tax_bins AS (\n        SELECT \n            FLOOR(mta_tax / 2) * 2 AS mta_tax_bin,\n            COUNT(*) AS bin_count\n        FROM \n            init_tlctrip\n        WHERE \n            mta_tax IS NOT NULL\n        GROUP BY \n            FLOOR(mta_tax / 2) * 2\n    ),\n    total_count AS (\n        SELECT \n            COUNT(*) AS total\n        FROM \n            init_tlctrip\n        WHERE \n            mta_tax IS NOT NULL\n    ),\n    cumulative_bins AS (\n        SELECT \n            mb.mta_tax_bin,\n            mb.bin_count,\n            SUM(mb.bin_count) OVER (ORDER BY mb.mta_tax_bin) AS cumulative_count\n        FROM \n            mta_tax_bins mb\n    )\n    SELECT \n        CONCAT('0-', mta_tax_bin + 1) AS mta_bin_range,\n        bin_count,\n        ROUND(cumulative_count * 100.0 / tc.total, 4) AS cumulative_percentage\n    FROM \n        cumulative_bins cb, total_count tc\n    ORDER BY \n        mta_tax_bin;\n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:46:10.036083Z","iopub.execute_input":"2024-07-02T20:46:10.036784Z","iopub.status.idle":"2024-07-02T20:46:53.172514Z","shell.execute_reply.started":"2024-07-02T20:46:10.036742Z","shell.execute_reply":"2024-07-02T20:46:53.171276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check tip_amount values distribution.\n\nconn.sql('''\n    WITH tip_bins AS (\n        SELECT \n            FLOOR(tip_amount / 10) * 10 AS tip_bin,\n            COUNT(*) AS bin_count\n        FROM \n            init_tlctrip\n        WHERE \n            tip_amount IS NOT NULL\n        GROUP BY \n            FLOOR(tip_amount / 10) * 10\n    ),\n    total_count AS (\n        SELECT \n            COUNT(*) AS total\n        FROM \n            init_tlctrip\n        WHERE \n            tip_amount IS NOT NULL\n    ),\n    cumulative_bins AS (\n        SELECT \n            tb.tip_bin,\n            tb.bin_count,\n            SUM(tb.bin_count) OVER (ORDER BY tb.tip_bin) AS cumulative_count\n        FROM \n            tip_bins tb\n    )\n    SELECT \n        CONCAT('0-', tip_bin + 9) AS tip_bin_range,\n        bin_count,\n        ROUND(cumulative_count * 100.0 / tc.total, 4) AS cumulative_percentage\n    FROM \n        cumulative_bins cb, total_count tc\n    ORDER BY \n        tip_bin;\n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:46:53.173869Z","iopub.execute_input":"2024-07-02T20:46:53.174203Z","iopub.status.idle":"2024-07-02T20:47:36.095139Z","shell.execute_reply.started":"2024-07-02T20:46:53.174174Z","shell.execute_reply":"2024-07-02T20:47:36.092618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check tolls_amount values distribution.\n\nconn.sql('''\n    WITH tolls_bins AS (\n        SELECT \n            FLOOR(tolls_amount / 10) * 10 AS tolls_bin,\n            COUNT(*) AS bin_count\n        FROM \n            init_tlctrip\n        WHERE \n            tolls_amount IS NOT NULL\n        GROUP BY \n            FLOOR(tolls_amount / 10) * 10\n    ),\n    total_count AS (\n        SELECT \n            COUNT(*) AS total\n        FROM \n            init_tlctrip\n        WHERE \n            tolls_amount IS NOT NULL\n    ),\n    cumulative_bins AS (\n        SELECT \n            tb.tolls_bin,\n            tb.bin_count,\n            SUM(tb.bin_count) OVER (ORDER BY tb.tolls_bin) AS cumulative_count\n        FROM \n            tolls_bins tb\n    )\n    SELECT \n        CONCAT('0-', tolls_bin + 9) AS tolls_bin_range,\n        bin_count,\n        ROUND(cumulative_count * 100.0 / tc.total, 4) AS cumulative_percentage\n    FROM \n        cumulative_bins cb, total_count tc\n    ORDER BY \n        tolls_bin;\n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:47:36.097552Z","iopub.execute_input":"2024-07-02T20:47:36.098358Z","iopub.status.idle":"2024-07-02T20:48:19.068036Z","shell.execute_reply.started":"2024-07-02T20:47:36.098309Z","shell.execute_reply":"2024-07-02T20:48:19.066082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check improvement_surchage values distribution.\n\nconn.sql('''\n    WITH improvement_bins AS (\n        SELECT \n            FLOOR(improvement_surcharge / 0.1) * 0.1 AS improvement_bin,\n            COUNT(*) AS bin_count\n        FROM \n            init_tlctrip\n        WHERE \n            improvement_surcharge IS NOT NULL\n        GROUP BY \n            FLOOR(improvement_surcharge / 0.1) * 0.1\n    ),\n    total_count AS (\n        SELECT \n            COUNT(*) AS total\n        FROM \n            init_tlctrip\n        WHERE \n            improvement_surcharge IS NOT NULL\n    ),\n    cumulative_bins AS (\n        SELECT \n            ib.improvement_bin,\n            ib.bin_count,\n            SUM(ib.bin_count) OVER (ORDER BY ib.improvement_bin) AS cumulative_count\n        FROM \n            improvement_bins ib\n    )\n    SELECT \n        CONCAT('0-', improvement_bin + 0.099) AS improvement_bin_range,\n        bin_count,\n        ROUND(cumulative_count * 100.0 / tc.total, 4) AS cumulative_percentage\n    FROM \n        cumulative_bins cb, total_count tc\n    ORDER BY \n        improvement_bin;\n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:48:19.070938Z","iopub.execute_input":"2024-07-02T20:48:19.07139Z","iopub.status.idle":"2024-07-02T20:49:02.582561Z","shell.execute_reply.started":"2024-07-02T20:48:19.071348Z","shell.execute_reply":"2024-07-02T20:49:02.581387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check total_amount values distribution.\n\nconn.sql('''\n    WITH total_amount_bins AS (\n        SELECT \n            FLOOR(total_amount / 50) * 50 AS total_amount_bin,\n            COUNT(*) AS bin_count\n        FROM \n            init_tlctrip\n        WHERE \n            total_amount IS NOT NULL\n        GROUP BY \n            FLOOR(total_amount / 50) * 50\n    ),\n    total_count AS (\n        SELECT \n            COUNT(*) AS total\n        FROM \n            init_tlctrip\n        WHERE \n            total_amount IS NOT NULL\n    ),\n    cumulative_bins AS (\n        SELECT \n            tab.total_amount_bin,\n            tab.bin_count,\n            SUM(tab.bin_count) OVER (ORDER BY tab.total_amount_bin) AS cumulative_count\n        FROM \n            total_amount_bins tab\n    )\n    SELECT \n        CONCAT('0-', total_amount_bin + 49) AS total_bin_range,\n        bin_count,\n        ROUND(cumulative_count * 100.0 / tc.total, 4) AS cumulative_percentage\n    FROM \n        cumulative_bins cb, total_count tc\n    ORDER BY \n        total_amount_bin;\n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:49:02.584111Z","iopub.execute_input":"2024-07-02T20:49:02.585223Z","iopub.status.idle":"2024-07-02T20:49:45.472615Z","shell.execute_reply.started":"2024-07-02T20:49:02.585179Z","shell.execute_reply":"2024-07-02T20:49:45.471632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check congestion_surcharge values distribution.\n\nconn.sql('''\n    WITH congestion_bins AS (\n        SELECT \n            FLOOR(congestion_surcharge / 0.25) * 0.25 AS congestion_bin,\n            COUNT(*) AS bin_count\n        FROM \n            init_tlctrip\n        WHERE \n            congestion_surcharge IS NOT NULL\n        GROUP BY \n            FLOOR(congestion_surcharge / 0.25) * 0.25\n    ),\n    total_count AS (\n        SELECT \n            COUNT(*) AS total\n        FROM \n            init_tlctrip\n        WHERE \n            congestion_surcharge IS NOT NULL\n    ),\n    cumulative_bins AS (\n        SELECT \n            cb.congestion_bin,\n            cb.bin_count,\n            SUM(cb.bin_count) OVER (ORDER BY cb.congestion_bin) AS cumulative_count\n        FROM \n            congestion_bins cb\n    )\n    SELECT \n        CONCAT('0-', congestion_bin + 0.249) AS congestion_bin_range,\n        bin_count,\n        ROUND(cumulative_count * 100.0 / tc.total, 4) AS cumulative_percentage\n    FROM \n        cumulative_bins cb, total_count tc\n    ORDER BY \n        congestion_bin;\n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:49:45.473731Z","iopub.execute_input":"2024-07-02T20:49:45.474712Z","iopub.status.idle":"2024-07-02T20:50:27.76219Z","shell.execute_reply.started":"2024-07-02T20:49:45.474677Z","shell.execute_reply":"2024-07-02T20:50:27.761077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check airport_fee values distribution.\n\nconn.sql('''\n    WITH airport_fee_bins AS (\n        SELECT \n            FLOOR(airport_fee / 0.25) * 0.25 AS airport_fee_bin,\n            COUNT(*) AS bin_count\n        FROM \n            init_tlctrip\n        WHERE \n            airport_fee IS NOT NULL\n        GROUP BY \n            FLOOR(airport_fee / 0.25) * 0.25\n    ),\n    total_count AS (\n        SELECT \n            COUNT(*) AS total\n        FROM \n            init_tlctrip\n        WHERE \n            airport_fee IS NOT NULL\n    ),\n    cumulative_bins AS (\n        SELECT \n            afb.airport_fee_bin,\n            afb.bin_count,\n            SUM(afb.bin_count) OVER (ORDER BY afb.airport_fee_bin) AS cumulative_count\n        FROM \n            airport_fee_bins afb\n    )\n    SELECT \n        CONCAT('0-', airport_fee_bin + 0.249) AS airport_bin_range,\n        bin_count,\n        ROUND(cumulative_count * 100.0 / tc.total, 4) AS cumulative_percentage\n    FROM \n        cumulative_bins cb, total_count tc\n    ORDER BY \n        airport_fee_bin;\n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:50:27.763842Z","iopub.execute_input":"2024-07-02T20:50:27.764177Z","iopub.status.idle":"2024-07-02T20:51:06.605114Z","shell.execute_reply.started":"2024-07-02T20:50:27.764148Z","shell.execute_reply":"2024-07-02T20:51:06.604249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.7: Assess Categorical Value Consistency","metadata":{}},{"cell_type":"code","source":"# Check if there is any odd value for 'store_and_fwd_flag' except N & Y.\n\nconn.sql('''\n    SELECT \n        DISTINCT store_and_fwd_flag, COUNT(*) AS count, ROUND(COUNT(*)*100/(SELECT COUNT('*') FROM init_tlctrip), 2) AS percentage\n    FROM \n        init_tlctrip\n    GROUP BY \n        1\n    ORDER BY \n        1\n''').to_df()\n# Check if there is any odd value for 'payment_type' except 1 to 6.\n\nconn.sql('''\n    SELECT \n        payment_type, COUNT(*), ROUND(COUNT(*)*100/(SELECT COUNT('*') FROM init_tlctrip), 2) AS percentage\n    FROM \n        init_tlctrip\n    GROUP BY \n        1\n    ORDER BY \n        1\n''').to_df()\n# Check if there is any odd value for 'VendorID' except 1 & 2.\n\nconn.sql('''\n    SELECT \n        cap_type, VendorID, COUNT(*), ROUND(COUNT(*)*100/(SELECT COUNT('*') FROM init_tlctrip), 2) AS percentage\n    FROM \n        init_tlctrip\n    GROUP BY \n        1, 2\n    ORDER BY \n        1, 2\n''').to_df()\n# Check if there is any odd value for 'RatecodeID' except 1 to 6.\n\nconn.sql('''\n    SELECT \n        RatecodeID, COUNT(*), ROUND(COUNT(*)*100/(SELECT COUNT('*') FROM init_tlctrip), 2) AS percentage\n    FROM \n        init_tlctrip\n    GROUP BY \n        1\n    ORDER BY\n        1\n''').to_df()\n# Check if there is any odd value for 'trip_type' except 1 or 2.\n\nconn.sql('''\n    SELECT \n        cap_type, trip_type, COUNT(*), ROUND(COUNT(*)*100/(SELECT COUNT('*') FROM init_tlctrip), 2) AS percentage\n    FROM \n        init_tlctrip\n    GROUP BY \n        1, 2\n    ORDER BY\n        1, 2\n''').to_df()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T06:23:44.12081Z","iopub.execute_input":"2024-07-03T06:23:44.121221Z","iopub.status.idle":"2024-07-03T06:27:22.438641Z","shell.execute_reply.started":"2024-07-03T06:23:44.121192Z","shell.execute_reply":"2024-07-03T06:27:22.437661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  1.2: Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"### 1.2.1: Identify Duplicate Records\n","metadata":{}},{"cell_type":"code","source":"# Check if there are duplicates.\n\nconn.sql('''\n    SELECT \n        VendorID, unified_pickup_datetime, unified_dropoff_datetime, trip_distance, PULocationID, DOLocationID, COUNT(*) AS number_of_rows, SUM(number_of_rows) OVER ()\n    FROM \n        init_tlctrip\n    GROUP BY \n        VendorID, unified_pickup_datetime, unified_dropoff_datetime, trip_distance, PULocationID, DOLocationID\n    HAVING \n        COUNT(*) > 1\n    ORDER BY unified_pickup_datetime,\n            unified_dropoff_datetime,\n            trip_distance,\n            PULocationID,\n            DOLocationID \n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:54:40.287064Z","iopub.execute_input":"2024-07-02T20:54:40.287444Z","iopub.status.idle":"2024-07-02T20:56:17.875743Z","shell.execute_reply.started":"2024-07-02T20:54:40.287386Z","shell.execute_reply":"2024-07-02T20:56:17.874221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2.2: Create Filtered View","metadata":{}},{"cell_type":"code","source":"# Create a new view due to outliers and out of range data. \n\nconn.sql('''\nCREATE OR REPLACE VIEW filtered_tlctrip AS\nWITH main_tlctrip AS (\n    SELECT \n        CAST(VendorID AS INT) AS VendorID,\n        CASE \n            WHEN store_and_fwd_flag = 'Y' THEN true\n            WHEN store_and_fwd_flag = 'N' THEN false\n            ELSE NULL\n        END AS store_and_fwd_flag,\n        CAST(RatecodeID AS INT) AS RatecodeID,\n        CAST(PULocationID AS INT) AS PULocationID,\n        CAST(DOLocationID AS INT) AS DOLocationID,\n        passenger_count,\n        trip_distance,\n        fare_amount,\n        extra,\n        mta_tax,\n        tip_amount,\n        tolls_amount,\n        improvement_surcharge,\n        total_amount,\n        payment_type,\n        trip_type,\n        congestion_surcharge,\n        airport_fee,\n        cap_type,\n        CAST(year AS INT) AS year,\n        CASE \n            WHEN cap_type = 'green' THEN lpep_pickup_datetime\n            WHEN cap_type = 'yellow' THEN tpep_pickup_datetime\n            ELSE NULL\n        END AS unified_pickup_datetime,\n        CASE \n            WHEN cap_type = 'green' THEN lpep_dropoff_datetime\n            WHEN cap_type = 'yellow' THEN tpep_dropoff_datetime\n            ELSE NULL\n        END AS unified_dropoff_datetime\n    FROM \n        read_parquet('/kaggle/input/tlc-trip-2020-01-to-2024-03/trips/*/*/*.parquet', hive_partitioning = true)\n    WHERE\n        unified_pickup_datetime < '2024-04-01' AND \n        unified_pickup_datetime >= '2020-01-01' AND\n        unified_pickup_datetime::TIMESTAMPTZ < unified_dropoff_datetime::TIMESTAMPTZ AND\n        payment_type NOT IN (0, 6) AND\n        VendorID NOT IN (5, 6) AND\n        RatecodeID != 99 AND\n        (fare_amount BETWEEN 0 AND 99 OR fare_amount IS NULL) AND\n        (extra BETWEEN 0 AND 10 OR extra IS NULL) AND\n        (mta_tax BETWEEN 0 AND 3 OR mta_tax IS NULL) AND\n        (tip_amount BETWEEN 0 AND 20 OR tip_amount IS NULL) AND\n        (tolls_amount BETWEEN 0 AND 20 OR tolls_amount IS NULL) AND\n        (passenger_count BETWEEN 1 AND 6 OR passenger_count IS NULL) AND\n        trip_distance > 0 AND\n        trip_distance <= 500 AND\n        trip_distance / (EXTRACT(EPOCH FROM unified_dropoff_datetime) - EXTRACT(EPOCH FROM unified_pickup_datetime)) * 3600 <= 90 AND\n        fare_amount >= 0 AND\n        total_amount >= 0 AND\n        extra >= 0 AND\n        mta_tax >= 0 AND\n        tip_amount >= 0 AND\n        tolls_amount >= 0 AND\n        (improvement_surcharge >= 0 OR improvement_surcharge IS NULL) AND\n        (congestion_surcharge >= 0 OR congestion_surcharge IS NULL) AND\n        (airport_fee >= 0 OR airport_fee IS NULL) AND\n        trip_distance > 0.31 AND\n        ((EXTRACT(EPOCH FROM unified_dropoff_datetime) - EXTRACT(EPOCH FROM unified_pickup_datetime)) > 60)\n)\nSELECT \n    tlc.VendorID,\n    tlc.store_and_fwd_flag,\n    tlc.RatecodeID,\n    tlc.PULocationID,\n    tlc.DOLocationID,\n    tlc.passenger_count,\n    tlc.trip_distance,\n    tlc.fare_amount,\n    tlc.extra,\n    tlc.mta_tax,\n    tlc.tip_amount,\n    tlc.tolls_amount,\n    tlc.improvement_surcharge,\n    tlc.total_amount,\n    tlc.payment_type,\n    tlc.trip_type,\n    tlc.congestion_surcharge,\n    tlc.airport_fee,\n    tlc.cap_type,\n    tlc.year,\n    tlc.unified_pickup_datetime,\n    tlc.unified_dropoff_datetime,\n    taxi.Zone AS PUZone,\n    CASE \n        WHEN tlc.DOLocationID = taxi.LocationID THEN taxi.Zone \n        ELSE (\n            SELECT Zone \n            FROM 'https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv' \n            WHERE LocationID = tlc.DOLocationID\n        ) \n    END AS DOZone\nFROM \n    main_tlctrip AS tlc\nLEFT JOIN (\n    SELECT \n        LocationID, \n        Zone \n    FROM \n        'https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv'\n) AS taxi\nON \n    tlc.PULocationID = taxi.LocationID;\n''')\n\n\nconn.sql('''\nDESCRIBE filtered_tlctrip\n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:19:19.07273Z","iopub.execute_input":"2024-07-02T22:19:19.073256Z","iopub.status.idle":"2024-07-02T22:19:20.317379Z","shell.execute_reply.started":"2024-07-02T22:19:19.073212Z","shell.execute_reply":"2024-07-02T22:19:20.316023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check filtered_tlctrip records count.\n\nconn.sql('''\nSELECT COUNT(*) FROM filtered_tlctrip\n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:56:19.160253Z","iopub.execute_input":"2024-07-02T20:56:19.16066Z","iopub.status.idle":"2024-07-02T20:58:23.831227Z","shell.execute_reply.started":"2024-07-02T20:56:19.160627Z","shell.execute_reply":"2024-07-02T20:58:23.829985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2.3: Exclude Duplicates","metadata":{}},{"cell_type":"code","source":"# Check if there are duplicates.\n\nconn.sql('''\n    SELECT \n        VendorID, unified_pickup_datetime, unified_dropoff_datetime, trip_distance, PULocationID, DOLocationID, COUNT(*) AS number_of_rows, SUM(number_of_rows) OVER ()\n    FROM \n        filtered_tlctrip\n    GROUP BY \n        VendorID, unified_pickup_datetime, unified_dropoff_datetime, trip_distance, PULocationID, DOLocationID\n    HAVING \n        COUNT(*) > 1\n    ORDER BY unified_pickup_datetime,\n            unified_dropoff_datetime,\n            trip_distance,\n            PULocationID,\n            DOLocationID \n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:19:45.287151Z","iopub.execute_input":"2024-07-02T22:19:45.287982Z","iopub.status.idle":"2024-07-02T22:22:57.967454Z","shell.execute_reply.started":"2024-07-02T22:19:45.287944Z","shell.execute_reply":"2024-07-02T22:22:57.965991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a new view without duplicates (there are only 52 rows that must be excluded).\n\nconn.sql('''\nCREATE OR REPLACE VIEW clean_tlctrip AS\nWITH ranked_trips AS (\n    SELECT\n        *,\n        ROW_NUMBER() OVER (PARTITION BY VendorID, unified_pickup_datetime, unified_dropoff_datetime, trip_distance, PULocationID, DOLocationID ORDER BY unified_pickup_datetime) AS rn\n    FROM\n        filtered_tlctrip\n)\nSELECT\n    VendorID,\n    store_and_fwd_flag,\n    RatecodeID,\n    PULocationID,\n    DOLocationID,\n    passenger_count,\n    trip_distance,\n    fare_amount,\n    extra,\n    mta_tax,\n    tip_amount,\n    tolls_amount,\n    improvement_surcharge,\n    total_amount,\n    payment_type,\n    trip_type,\n    congestion_surcharge,\n    airport_fee,\n    cap_type,\n    year,\n    unified_pickup_datetime,\n    unified_dropoff_datetime,\n    PUZone,\n    DOZone, \nFROM\n    ranked_trips\nWHERE\n    rn = 1;\n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:22:57.970049Z","iopub.execute_input":"2024-07-02T22:22:57.97058Z","iopub.status.idle":"2024-07-02T22:22:59.222109Z","shell.execute_reply.started":"2024-07-02T22:22:57.970536Z","shell.execute_reply":"2024-07-02T22:22:59.220892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check tlctrip records count after excluding duplicates.\n\nconn.sql('''\nSELECT COUNT(*) FROM clean_tlctrip\n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:01:26.470922Z","iopub.execute_input":"2024-07-02T21:01:26.471263Z","iopub.status.idle":"2024-07-02T21:04:28.943043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2.4: Save Filtered_tlcTrip to a Parquet File","metadata":{}},{"cell_type":"code","source":"# # Define the path where you want to save the Parquet file\noutput_parquet_path = '/kaggle/working/tlctrip3.parquet'\n\n # Execute the SQL query to create and save the Parquet file directly\nconn.sql(f'''\n COPY (SELECT * FROM filtered_tlctrip) TO '{output_parquet_path}' (FORMAT PARQUET);\n ''')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:11:24.888817Z","iopub.execute_input":"2024-07-02T21:11:24.889731Z","iopub.status.idle":"2024-07-02T21:19:30.249725Z","shell.execute_reply.started":"2024-07-02T21:11:24.889693Z","shell.execute_reply":"2024-07-02T21:19:30.248355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conn.sql('''\nCREATE OR REPLACE VIEW tlctrip AS\nSELECT \n    *\nFROM \n    read_parquet('/kaggle/input/final-tlctrip-zone-view/tlctrip3.parquet', hive_partitioning = true)\n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:48:09.257584Z","iopub.execute_input":"2024-07-03T05:48:09.258001Z","iopub.status.idle":"2024-07-03T05:48:09.360667Z","shell.execute_reply.started":"2024-07-03T05:48:09.257969Z","shell.execute_reply":"2024-07-03T05:48:09.359361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check tlctrip records count.\n\nconn.sql('''\nSELECT COUNT(*) FROM tlctrip\n''')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get a look on fields statistical summary.\n\nconn.sql('''\nSUMMARIZE tlctrip\n''').df()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:19:36.655958Z","iopub.execute_input":"2024-07-02T21:19:36.658952Z","iopub.status.idle":"2024-07-02T21:23:27.28193Z","shell.execute_reply.started":"2024-07-02T21:19:36.658722Z","shell.execute_reply":"2024-07-02T21:23:27.280384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3: Visualization","metadata":{}},{"cell_type":"code","source":"# Run the SQL query and print the resulting DataFrame\ndf = conn.sql('''\nSELECT \n    date_trunc('month', unified_pickup_datetime) as unified_pickup_month,\n    cap_type,\n    COUNT(*) AS trip_count,\n    AVG(fare_amount/trip_distance) AS USD_per_mile,\n    AVG(fare_amount/(trip_distance*passenger_count)) AS USD_per_mile_person,\n    AVG(total_amount) as avg_total_amount\nFROM \n    tlctrip\nGROUP BY\n    1, 2\nORDER BY\n    1, 2\n''').df()\n\n# Check if the DataFrame is not empty\nprint(df.head())\n\n# Define the color map for the lines\ncolor_map = {'green': 'green', 'yellow': 'darkgoldenrod'}\n\n# Ensure the DataFrame is not empty\nif not df.empty:\n    # Convert the unified_pickup_month to datetime for proper plotting\n    df['unified_pickup_month'] = pd.to_datetime(df['unified_pickup_month'])\n\n    # Plot each figure with updated axis labels\n    fig = px.line(df, x='unified_pickup_month', y='trip_count', color='cap_type', title='Trip Count per Month', \n                  color_discrete_map=color_map)\n    fig.update_xaxes(title_text='Date', dtick=\"M4\", tickformat=\"%b %Y\")  # Update x-axis to show every 3 months\n    fig.update_yaxes(title_text='Trip Count')\n    fig.show()\n\n    fig = px.line(df, x='unified_pickup_month', y='avg_total_amount', color='cap_type', title='Average Total Amount', \n                  color_discrete_map=color_map)\n    fig.update_xaxes(title_text='Date', dtick=\"M4\", tickformat=\"%b %Y\")  # Update x-axis to show every 3 months\n    fig.update_yaxes(title_text='Average Total Amount ($)')\n    fig.show()\n\n    fig = px.line(df, x='unified_pickup_month', y='USD_per_mile', color='cap_type', title='Fare Amount per Mile', \n                  color_discrete_map=color_map)\n    fig.update_xaxes(title_text='Date', dtick=\"M4\", tickformat=\"%b %Y\")  # Update x-axis to show every 3 months\n    fig.update_yaxes(title_text='Cost per Mile ($)')\n    fig.show()\n\n    fig = px.line(df, x='unified_pickup_month', y='USD_per_mile_person', color='cap_type', title='Fare Amount per Mile-Person', \n                  color_discrete_map=color_map)\n    fig.update_xaxes(title_text='Date', dtick=\"M4\", tickformat=\"%b %Y\")  # Update x-axis to show every 3 months\n    fig.update_yaxes(title_text='Cost per Mile Person ($)')\n    fig.show()\nelse:\n    print(\"The DataFrame is empty.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:23:27.284708Z","iopub.execute_input":"2024-07-02T21:23:27.285242Z","iopub.status.idle":"2024-07-02T21:23:42.403825Z","shell.execute_reply.started":"2024-07-02T21:23:27.285187Z","shell.execute_reply":"2024-07-02T21:23:42.402666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run the SQL query and print the resulting DataFrame\ndf = conn.sql('''\nSELECT \n    date_trunc('month', unified_pickup_datetime) as unified_pickup_month,\n    cap_type,\n    COUNT(*) AS trip_count,\n    AVG(fare_amount/trip_distance) AS USD_per_mile,\n    AVG(fare_amount/(trip_distance*passenger_count)) AS USD_per_mile_person,\n    AVG(total_amount) as avg_total_amount\nFROM \n    tlctrip\nGROUP BY\n    1, 2\nORDER BY\n    1, 2\n''').df()\n\n# Check if the DataFrame is not empty\nprint(df.head())\n\n# Define the color map for the lines\ncolor_map = {'green': 'green', 'yellow': 'darkgoldenrod'}\n\n# Ensure the DataFrame is not empty\nif not df.empty:\n    # Convert the unified_pickup_month to datetime for proper plotting\n    df['unified_pickup_month'] = pd.to_datetime(df['unified_pickup_month'])\n\n    # Plot each figure with updated axis labels\n    fig = px.line(df, x='unified_pickup_month', y='trip_count', color='cap_type', title='Trip Count per Month', \n                  color_discrete_map=color_map)\n    fig.update_xaxes(title_text='Date', dtick=\"M4\", tickformat=\"%b %Y\")  # Update x-axis to show every 3 months\n    fig.update_yaxes(title_text='Trip Count')\n    fig.show()\n\n    fig = px.line(df, x='unified_pickup_month', y='avg_total_amount', color='cap_type', title='Average Total Amount', \n                  color_discrete_map=color_map)\n    fig.update_xaxes(title_text='Date', dtick=\"M4\", tickformat=\"%b %Y\")  # Update x-axis to show every 3 months\n    fig.update_yaxes(title_text='Average Total Amount ($)')\n    fig.show()\n\n    fig = px.line(df, x='unified_pickup_month', y='USD_per_mile', color='cap_type', title='Fare Amount per Mile', \n                  color_discrete_map=color_map)\n    fig.update_xaxes(title_text='Date', dtick=\"M4\", tickformat=\"%b %Y\")  # Update x-axis to show every 3 months\n    fig.update_yaxes(title_text='Cost per Mile ($)')\n    fig.show()\n\n    fig = px.line(df, x='unified_pickup_month', y='USD_per_mile_person', color='cap_type', title='Fare Amount per Mile-Person', \n                  color_discrete_map=color_map)\n    fig.update_xaxes(title_text='Date', dtick=\"M4\", tickformat=\"%b %Y\")  # Update x-axis to show every 3 months\n    fig.update_yaxes(title_text='Cost per Mile Person ($)')\n    fig.show()\nelse:\n    print(\"The DataFrame is empty.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:23:42.405313Z","iopub.execute_input":"2024-07-02T21:23:42.405908Z","iopub.status.idle":"2024-07-02T21:23:53.814063Z","shell.execute_reply.started":"2024-07-02T21:23:42.405876Z","shell.execute_reply":"2024-07-02T21:23:53.812969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Take a look on payment_type distribution.\n# Example SQL query using DuckDB\nquery = '''\n    SELECT payment_type, COUNT(*) as count, (COUNT(*) * 100.0 / (SELECT COUNT(*) FROM tlctrip)) AS percentage\n    FROM tlctrip\n    GROUP BY payment_type\n'''\n\n# Connect to DuckDB and execute the query\ndf = conn.execute(query).fetchdf()\n\n# Convert query result to a pandas DataFrame\ndf.columns = ['payment_type', 'count', 'percentage']\n\n# Save data to CSV file\ndf[['payment_type', 'percentage']].to_csv('payment_type_percentages.csv', index=False)\n\n# Display the DataFrame (optional)\nprint(df)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T06:28:27.136452Z","iopub.execute_input":"2024-07-03T06:28:27.136912Z","iopub.status.idle":"2024-07-03T06:28:30.969209Z","shell.execute_reply.started":"2024-07-03T06:28:27.136868Z","shell.execute_reply":"2024-07-03T06:28:30.967968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Q.2**\n   * **2.1: Selecting Data**\n        \n   * **2.2: Market Share Chart**\n   \n   * **2.3: Calculating Profit**\n   \n   * **2.4: Calculating Profit**\n   \n   * **2.4: Calculation of market volume in previous years**\n   \n   * **2.5: Graph of market volume growth rate in recent years**\n   \n   ","metadata":{}},{"cell_type":"markdown","source":"## 2.1: Selecting Data","metadata":{}},{"cell_type":"code","source":"conn.sql('''\nCREATE OR REPLACE VIEW tlctrip AS \nSELECT *\nFROM '/kaggle/input/final-tlctrip-zone-view/tlctrip3.parquet'\n''')\nconn.sql('''\nCREATE OR REPLACE VIEW new_tlctrip AS\nSELECT \n     PUZone as zone\n    ,PUlocationID AS start_location_id \n    ,count(*) AS number_of_rides\n    ,sum(total_amount) AS sum_of_price\n    ,percentile_disc(0.50) WITHIN GROUP (ORDER BY total_amount) AS price_quartile_50\n    ,sum_of_price/(SELECT SUM(total_amount) FROM tlctrip WHERE unified_pickup_datetime BETWEEN '2023-04-01 00:00:00' AND '2024-03-31 00:00:00' AND payment_type=1)*100 AS market_cap_percentage\n    ,number_of_rides * price_quartile_50 AS metric\nFROM tlctrip\nWHERE \n    unified_pickup_datetime BETWEEN '2023-04-01 00:00:00' AND '2024-03-31 00:00:00'  \n    AND payment_type=1\nGROUP BY 1,2\nORDER BY 7 desc\n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:32:00.393902Z","iopub.execute_input":"2024-07-02T21:32:00.394357Z","iopub.status.idle":"2024-07-02T21:32:00.631598Z","shell.execute_reply.started":"2024-07-02T21:32:00.394322Z","shell.execute_reply":"2024-07-02T21:32:00.630261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df21 = conn.sql('''\nselect sum(market_cap_percentage) from new_tlctrip\n''').to_df()\ndf21","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:32:17.285472Z","iopub.execute_input":"2024-07-02T21:32:17.285867Z","iopub.status.idle":"2024-07-02T21:32:23.209766Z","shell.execute_reply.started":"2024-07-02T21:32:17.285838Z","shell.execute_reply":"2024-07-02T21:32:23.208567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df22 = conn.sql('''\nSELECT \n*\n,SUM(market_cap_percentage) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_market_cap_percentage\nFROM new_tlctrip\nLIMIT 12\n''').to_df()\ndf22","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:34:04.220037Z","iopub.execute_input":"2024-07-02T21:34:04.220515Z","iopub.status.idle":"2024-07-02T21:34:10.014122Z","shell.execute_reply.started":"2024-07-02T21:34:04.220481Z","shell.execute_reply":"2024-07-02T21:34:10.012799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2:Market share chart","metadata":{}},{"cell_type":"code","source":"df_sorted = df22.sort_values(by='market_cap_percentage', ascending=True)\n\n# Create the horizontal bar chart using px.bar\nfig = px.bar(\n    df_sorted,  # Data source (sorted DataFrame)\n    x='market_cap_percentage',  # x-axis (market_cap_percentage)\n    y='zone',  # y-axis (zone)\n    title='Market Capacity Percentage by Zone In a Recent Year',\n    orientation='h',  # Horizontal orientation\n    #color='zone',  # Color based on market_cap_percentage\n    text=df_sorted['market_cap_percentage'].round(1),  # Display values as text on bars\n    labels={  # Customize axis labels\n        'market_cap_percentage': 'Market Cap Percentage (%)',\n        'zone': 'Zone'\n    }\n)\n\n# Display the chart\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:34:30.507792Z","iopub.execute_input":"2024-07-02T21:34:30.508238Z","iopub.status.idle":"2024-07-02T21:34:30.614095Z","shell.execute_reply.started":"2024-07-02T21:34:30.508207Z","shell.execute_reply":"2024-07-02T21:34:30.612698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3:Calculating Profit","metadata":{}},{"cell_type":"code","source":"df23 = conn.sql('''\nSELECT \n    SUM(total_amount)*(52/100)*(10/100)*(1.5/100) AS benefit\nFROM tlctrip \nWHERE \n    unified_pickup_datetime BETWEEN '2023-04-01 00:00:00' AND '2024-03-31 00:00:00'  \n    AND payment_type=1\n''').to_df()\ndf23","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:34:36.89331Z","iopub.execute_input":"2024-07-02T21:34:36.89376Z","iopub.status.idle":"2024-07-02T21:34:38.758095Z","shell.execute_reply.started":"2024-07-02T21:34:36.893726Z","shell.execute_reply":"2024-07-02T21:34:38.75703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.4:Calculation of market volume in previous years","metadata":{}},{"cell_type":"code","source":"df24 = conn.sql('''\nSELECT \n    Case\n        WHEN unified_pickup_datetime BETWEEN '2023-04-01 00:00:00' AND '2024-04-01 00:00:00' THEN '2023-2024'\n        WHEN unified_pickup_datetime BETWEEN '2022-04-01 00:00:00' AND '2023-03-31 00:00:00' THEN '2022-2023'\n        WHEN unified_pickup_datetime BETWEEN '2021-04-01 00:00:00' AND '2022-03-31 00:00:00' THEN '2021-2022'\n        WHEN unified_pickup_datetime BETWEEN '2020-04-01 00:00:00' AND '2021-03-31 00:00:00' THEN '2020-2021'\n    ELSE '0'\n    END AS year_period\n    ,Round(SUM(total_amount) ,1) AS market_cap\n    ,(market_cap - LAG(market_cap) OVER (ORDER BY year_period)) / LAG(market_cap) OVER (ORDER BY year_period) * 100 AS market_cap_growth_rate_percentage\nFROM tlctrip \nWHERE \n     payment_type = 1\n     AND year_period != '0'\nGROUP BY 1\norder by 1 \n''').to_df()\ndf24","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:34:42.993386Z","iopub.execute_input":"2024-07-02T21:34:42.993824Z","iopub.status.idle":"2024-07-02T21:34:49.457613Z","shell.execute_reply.started":"2024-07-02T21:34:42.993793Z","shell.execute_reply":"2024-07-02T21:34:49.456287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Graph of market volume growth rate in recent years","metadata":{}},{"cell_type":"code","source":"df24 = df24.sort_values(by='market_cap', ascending=True)\nfig = fig = px.bar(\n    data_frame=df24,  # Data source (DataFrame)\n    x='year_period',  # x-axis (year_period)\n    y='market_cap',  # y-axis (market_cap)\n    title='Market Capacity In Recent Years',\n    text='market_cap',  # Display values as text on bars\n    labels={  # Customize axis labels\n        'market_cap': 'Market Capacity',\n        'year_period': 'Year Period'\n    }\n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:37:25.704633Z","iopub.execute_input":"2024-07-02T21:37:25.705511Z","iopub.status.idle":"2024-07-02T21:37:25.792789Z","shell.execute_reply.started":"2024-07-02T21:37:25.705465Z","shell.execute_reply":"2024-07-02T21:37:25.791595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract data for each plot and save to CSV files\nplot_data = {\n    'Market Capacity In Recent Years': df24[['year_period', 'market_cap']],\n    'Market Capacity Percentage by Zone In a Recent Year': df22[['zone', 'market_cap_percentage']],\n}\n\nfor title, data in plot_data.items():\n    csv_filename = title.replace(' ', '_').lower() + '.csv'\n    data.to_csv(csv_filename, index=False)\n    print(f'Data for \"{title}\" saved to {csv_filename}')\nelse:\n    print(\"The DataFrame is empty.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:37:33.902851Z","iopub.execute_input":"2024-07-02T21:37:33.903986Z","iopub.status.idle":"2024-07-02T21:37:33.917112Z","shell.execute_reply.started":"2024-07-02T21:37:33.903946Z","shell.execute_reply":"2024-07-02T21:37:33.91586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Q.3**\n   * **3.1: Annual Duration Times Passengers**\n\n   * **3.2: Calculate the Duration Times Passengers for the Top 5 Zones**","metadata":{}},{"cell_type":"markdown","source":"## 3.1: Annual Duration Times Passengers","metadata":{}},{"cell_type":"code","source":"# Calculate the Annual Duration Times Passengers (DTP), tlcTrip revenue & advertising company gross revenue for the last 12 months.\n\nconn.sql('''\nWITH trip_details AS (\n  SELECT\n    EXTRACT(EPOCH FROM unified_dropoff_datetime) - EXTRACT(EPOCH FROM unified_pickup_datetime) AS trip_duration_seconds,\n    passenger_count\n  FROM\n    tlctrip\n  WHERE unified_pickup_datetime BETWEEN '2023-04-01 00:00:00' AND '2024-03-31 23:59:59'\n),\nagg_trip_details AS (\n  SELECT\n    SUM(trip_duration_seconds) AS total_trip_duration_seconds,\n    SUM(passenger_count) AS total_passengers\n  FROM\n    trip_details\n)\nSELECT\n  'All Zones' AS Zone,\n  total_passengers AS annual_passengers,\n  CONCAT(\n    (total_trip_duration_seconds / 3600)::INT, ':',\n    LPAD(((total_trip_duration_seconds % 3600) / 60)::INT::TEXT, 2, '0'), ':',\n    LPAD((total_trip_duration_seconds % 60)::INT::TEXT, 2, '0')\n  ) AS annual_trip_duration,\n  ROUND((total_trip_duration_seconds * 0.001765 / 30), 2) AS annual_expected_tlc_revenue_dollar,\n  ROUND(((total_trip_duration_seconds * 0.001765 / 30) * 0.15), 2) AS annual_adv_gross_revenue_dollar\nFROM\n  agg_trip_details;\n''').df()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:37:43.597625Z","iopub.execute_input":"2024-07-02T21:37:43.59806Z","iopub.status.idle":"2024-07-02T21:37:45.578601Z","shell.execute_reply.started":"2024-07-02T21:37:43.598027Z","shell.execute_reply":"2024-07-02T21:37:45.577474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.2: Calculate the Duration Times Passengers for the Top 5 Zones\n","metadata":{}},{"cell_type":"code","source":"# Calculate the Duration Times Passengers (DTP), tlcTrip revenue & advertising company gross revenue for the Top 5 Zones in the last 3 months.\n\nconn.sql('''\nWITH trip_details AS (\n  SELECT\n    EXTRACT(EPOCH FROM unified_dropoff_datetime) - EXTRACT(EPOCH FROM unified_pickup_datetime) AS trip_duration_seconds,\n    passenger_count,\n    PUZone\n  FROM\n    tlctrip\n  WHERE unified_pickup_datetime BETWEEN '2024-01-01 00:00:00' AND '2024-03-31 23:59:59'\n),\nagg_trip_details AS (\n  SELECT\n    SUM(trip_duration_seconds) AS total_trip_duration_seconds,\n    SUM(passenger_count) AS total_passengers,\n    PUZone\n  FROM\n    trip_details\n  GROUP BY\n    PUZone\n),\nformatted_trip_duration AS (\n  SELECT\n    PUZone,\n    total_passengers,\n    total_trip_duration_seconds,\n    (total_trip_duration_seconds / 3600)::INT AS hours,\n    ((total_trip_duration_seconds % 3600) / 60)::INT AS minutes,\n    (total_trip_duration_seconds % 60)::INT AS seconds\n  FROM\n    agg_trip_details\n),\ntop_5_zones AS (\n  SELECT\n    PUZone,\n    total_passengers,\n    total_trip_duration_seconds,\n    (total_trip_duration_seconds / 3600)::INT AS hours,\n    ((total_trip_duration_seconds % 3600) / 60)::INT AS minutes,\n    (total_trip_duration_seconds % 60)::INT AS seconds\n  FROM\n    formatted_trip_duration\n  ORDER BY\n    total_trip_duration_seconds DESC\n  LIMIT 5\n)\nSELECT\n  PUZone,\n  total_passengers AS passengers,\n  CONCAT(hours::TEXT, ':', LPAD(minutes::TEXT, 2, '0'), ':', LPAD(seconds::TEXT, 2, '0')) AS trip_duration,\n  ROUND((total_trip_duration_seconds * 0.001765 / 30), 2) AS expected_tlc_revenue_dollar,\n  ROUND(((total_trip_duration_seconds * 0.001765 / 30) * 0.15), 2) AS adv_gross_revenue_dollar\nFROM\n  top_5_zones\n\nUNION ALL\n\nSELECT\n  'Total of TOP 5' AS PUZone,\n  SUM(total_passengers) AS passengers,\n  CONCAT(\n    (SUM(total_trip_duration_seconds) / 3600)::INT, ':',\n    LPAD(((SUM(total_trip_duration_seconds) % 3600) / 60)::INT::TEXT, 2, '0'), ':',\n    LPAD((SUM(total_trip_duration_seconds) % 60)::INT::TEXT, 2, '0')\n  ) AS trip_duration,\n  ROUND(SUM((total_trip_duration_seconds) * 0.001765 / 30), 2) AS expected_tlc_revenue_dollar,\n  ROUND(SUM((total_trip_duration_seconds * 0.001765 / 30) * 0.15), 2) AS adv_gross_revenue_dollar\nFROM\n  top_5_zones;\n''').df()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:37:48.156784Z","iopub.execute_input":"2024-07-02T21:37:48.157692Z","iopub.status.idle":"2024-07-02T21:37:50.524987Z","shell.execute_reply.started":"2024-07-02T21:37:48.157652Z","shell.execute_reply":"2024-07-02T21:37:50.523716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example SQL query using DuckDB (assuming df32 is your DuckDB result)\nquery = '''\n    WITH trip_details AS (\n      SELECT\n        EXTRACT(EPOCH FROM unified_dropoff_datetime) - EXTRACT(EPOCH FROM unified_pickup_datetime) AS trip_duration_seconds,\n        passenger_count,\n        PUZone\n      FROM\n        tlctrip\n      WHERE unified_pickup_datetime BETWEEN '2024-01-01 00:00:00' AND '2024-03-31 23:59:59'\n    ),\n    agg_trip_details AS (\n      SELECT\n        SUM(trip_duration_seconds) AS total_trip_duration_seconds,\n        SUM(passenger_count) AS total_passengers,\n        PUZone\n      FROM\n        trip_details\n      GROUP BY\n        PUZone\n    ),\n    formatted_trip_duration AS (\n      SELECT\n        PUZone,\n        total_passengers,\n        total_trip_duration_seconds,\n        (total_trip_duration_seconds / 3600)::INT AS hours,\n        ((total_trip_duration_seconds % 3600) / 60)::INT AS minutes,\n        (total_trip_duration_seconds % 60)::INT AS seconds\n      FROM\n        agg_trip_details\n    ),\n    top_5_zones AS (\n      SELECT\n        PUZone,\n        total_passengers,\n        total_trip_duration_seconds,\n        (total_trip_duration_seconds / 3600)::INT AS hours,\n        ((total_trip_duration_seconds % 3600) / 60)::INT AS minutes,\n        (total_trip_duration_seconds % 60)::INT AS seconds\n      FROM\n        formatted_trip_duration\n      ORDER BY\n        total_trip_duration_seconds DESC\n      LIMIT 5\n    )\n    SELECT\n      PUZone,\n      total_passengers AS passengers,\n      CONCAT(hours::TEXT, ':', LPAD(minutes::TEXT, 2, '0'), ':', LPAD(seconds::TEXT, 2, '0')) AS trip_duration,\n      ROUND((total_trip_duration_seconds * 0.001765 / 30), 2) AS expected_tlc_revenue_dollar,\n      ROUND(((total_trip_duration_seconds * 0.001765 / 30) * 0.15), 2) AS adv_gross_revenue_dollar\n    FROM\n      top_5_zones\n\n    UNION ALL\n\n    SELECT\n      'Total of TOP 5' AS PUZone,\n      SUM(total_passengers) AS passengers,\n      CONCAT(\n        (SUM(total_trip_duration_seconds) / 3600)::INT, ':',\n        LPAD(((SUM(total_trip_duration_seconds) % 3600) / 60)::INT::TEXT, 2, '0'), ':',\n        LPAD((SUM(total_trip_duration_seconds) % 60)::INT::TEXT, 2, '0')\n      ) AS trip_duration,\n      ROUND(SUM((total_trip_duration_seconds) * 0.001765 / 30), 2) AS expected_tlc_revenue_dollar,\n      ROUND(SUM((total_trip_duration_seconds * 0.001765 / 30) * 0.15), 2) AS adv_gross_revenue_dollar\n    FROM\n      top_5_zones;\n'''\n\n# Connect to DuckDB and execute the query\ndf32 = conn.sql(query).df()\n\n# Melt the DataFrame to convert columns into a long format\ndf32_melted = pd.melt(df32, id_vars=['PUZone'], value_vars=['expected_tlc_revenue_dollar', 'adv_gross_revenue_dollar'],\n                      var_name='Revenue Type', value_name='Revenue')\n\n# Customizing the names as per your requirement\ndf32_melted['Revenue Type'] = df32_melted['Revenue Type'].replace({\n    'expected_tlc_revenue_dollar': 'tlctrip_revenue',\n    'adv_gross_revenue_dollar': 'advertising_gross_revenue'\n})\n\n# Pivot the melted DataFrame so each PUZone has both revenue types as columns\ndf32_pivot = df32_melted.pivot(index='PUZone', columns='Revenue Type', values='Revenue').reset_index()\n\n# Define colors for each Revenue Type (adjusting blue to be darker)\ncolors = {'tlctrip_revenue': 'deepskyblue', 'advertising_gross_revenue': 'orange'}\n\n# Plot using Plotly Express\nfig = px.bar(df32_pivot, x='PUZone', y=['tlctrip_revenue', 'advertising_gross_revenue'],\n             barmode='group',  # Clustered bar mode\n             color_discrete_map=colors,\n             labels={'PUZone': 'PUZone', 'value': 'Revenue (Dollars)', 'variable': 'Revenue Type'})\n\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:37:54.182749Z","iopub.execute_input":"2024-07-02T21:37:54.183179Z","iopub.status.idle":"2024-07-02T21:37:57.096559Z","shell.execute_reply.started":"2024-07-02T21:37:54.183146Z","shell.execute_reply":"2024-07-02T21:37:57.095221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Q.4**\n   * **4.1: Selecting & Sampling Data**\n        \n   * **4.2: Apply the coefficients to calculat**\n   \n   * **4.3: OLS Model**\n   \n   * **4.4: Visualisation**\n   \n   \n   ","metadata":{}},{"cell_type":"markdown","source":"## 4.1: Selecting & Sampling Data","metadata":{}},{"cell_type":"code","source":"# Create 10 percent sample from unique_tlctrip\nquery = '''\nSELECT \n    total_amount,\n    trip_distance, \n    passenger_count, \n    RatecodeID, \n    cap_type,  \n    year, \n    unified_pickup_datetime,\nFROM\n    read_parquet('/kaggle/input/final-tlctrip-zone-view/tlctrip3.parquet')\nUSING SAMPLE 10 PERCENT\n'''\nsmpl = conn.sql(query).df()\nsmpl\nsmpl.describe().applymap('{:,.2f}'.format)\nsmpl.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:24:47.805836Z","iopub.execute_input":"2024-07-02T22:24:47.806382Z","iopub.status.idle":"2024-07-02T22:25:08.99022Z","shell.execute_reply.started":"2024-07-02T22:24:47.806341Z","shell.execute_reply":"2024-07-02T22:25:08.988861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of columns to check for missing values\ncolumns_to_check = ['total_amount', 'trip_distance', 'passenger_count', 'RatecodeID', 'cap_type', 'year', 'unified_pickup_datetime']\n\n# Calculate the number of missing values in each specified column\nmissing_values = smpl[columns_to_check].isnull().sum()\nmissing_values\n\n#No missing, so there was no need to drop them\n\n# Drop rows where any of these columns have missing values\n# columns_to_clean = ['passenger_count', 'RatecodeID']\n# clnd_smpl_10prcnt = smpl_10prcnt.dropna(subset=columns_to_clean)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:38:31.446374Z","iopub.execute_input":"2024-07-02T21:38:31.44683Z","iopub.status.idle":"2024-07-02T21:38:33.600486Z","shell.execute_reply.started":"2024-07-02T21:38:31.446795Z","shell.execute_reply":"2024-07-02T21:38:33.599134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2: Apply the coefficients to calculat","metadata":{}},{"cell_type":"code","source":"#Build required variables and modify name of the current ones\nsmpl.rename(columns={'cap_type': 'cab_type'}, inplace=True)\nsmpl.rename(columns={'trip_distance': 'distance'}, inplace=True)\nsmpl.rename(columns={'total_amount': 'fare'}, inplace=True)\n\n#Creat 'real_price' which gives prices in 2024\n# Convert 'year' column to datetime\nsmpl['dt_year'] = pd.to_datetime(smpl['year'], format='%Y')\n# Create a dictionary of coefficients for each year\ncoefficients = {\n    2020: 1.21,\n    2021: 1.16,\n    2022: 1.07,\n    2023: 1.03,\n    2024: 1.00\n}\n\n# Apply the coefficients to calculate 'real_price'\nsmpl['rlfare'] = smpl.apply(lambda row: row['fare'] * coefficients.get(row['dt_year'].year, 1.0), axis=1)\nsmpl['lnrlfare'] = np.log(smpl['rlfare'])\n\nsmpl['hour'] = smpl['unified_pickup_datetime'].dt.hour\nsmpl['weekday'] = smpl['unified_pickup_datetime'].dt.day_name()\nsmpl['month'] = smpl['unified_pickup_datetime'].dt.month_name()\n\nsmpl['cab_type'] = smpl['cab_type'].astype('category')\nsmpl['RatecodeID'] = smpl['RatecodeID'].astype('category')\nsmpl['hour'] = smpl['hour'].astype('category')\nsmpl['weekday'] = smpl['weekday'].astype('category')\nsmpl['month'] = smpl['month'].astype('category')\nsmpl['year'] = smpl['year'].astype('category')\n\n# Remove rows with infinite values if any\nsmpl = smpl.replace([np.inf, -np.inf], np.nan).dropna()\n\nreg_df = smpl[['lnrlfare', 'distance', 'passenger_count', 'RatecodeID', 'cab_type', 'hour', 'weekday', 'month', 'year']]\nreg_df.head(5)\nreg_df.info()\nreg_df.describe().applymap('{:,.2f}'.format)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:25:31.014861Z","iopub.execute_input":"2024-07-02T22:25:31.015358Z","iopub.status.idle":"2024-07-02T22:31:01.793924Z","shell.execute_reply.started":"2024-07-02T22:25:31.015322Z","shell.execute_reply":"2024-07-02T22:31:01.792459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3: OLS Model","metadata":{}},{"cell_type":"code","source":"formula = 'lnrlfare ~ distance + passenger_count + C(RatecodeID) + C(cab_type) + C(hour) + C(weekday) + C(month) + C(year)'\nmodel = sm.OLS.from_formula(formula, data=reg_df).fit()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:05:29.503124Z","iopub.execute_input":"2024-07-02T22:05:29.503587Z","iopub.status.idle":"2024-07-02T22:11:23.13102Z","shell.execute_reply.started":"2024-07-02T22:05:29.50355Z","shell.execute_reply":"2024-07-02T22:11:23.129581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Notes:\n* Standard Errors assume that the covariance matrix of the errors is correctly specified.\n* The condition number is large, 5.66e+03. This might indicate that there are strong multicollinearity or other numerical problems.","metadata":{}},{"cell_type":"code","source":"# model = sm.OLS.from_formula(\"lnprice ~ distance + passenger_count + C(RatecodeID) + C(cab_type) + C(hour) + C(weekday) + C(month) + C(year)\", data=reg_df).fit()\nformula = 'lnrlfare ~ distance + passenger_count + C(RatecodeID) + C(cab_type) + C(hour) + C(weekday) + C(month) + C(year)'\nmodel = sm.OLS.from_formula(formula, data=reg_df).fit()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:36:43.303595Z","iopub.execute_input":"2024-07-02T22:36:43.305478Z","iopub.status.idle":"2024-07-02T22:42:28.560744Z","shell.execute_reply.started":"2024-07-02T22:36:43.305429Z","shell.execute_reply":"2024-07-02T22:42:28.559326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract coefficients of the regression\ncoef_df = model.params.reset_index()\ncoef_df.columns = ['variable', 'coefficient']\ncoef_df\ncoef_df.info()\n\n# Extract statistics\nstderr_df = model.bse.reset_index()\nstderr_df.columns = ['variable', 'std_err']\n\ntvalues_df = model.tvalues.reset_index()\ntvalues_df.columns = ['variable', 'tvalue']\n\npvalues_df = model.pvalues.reset_index()\npvalues_df.columns = ['variable', 'pvalue']\n\nconf_int_df = model.conf_int().reset_index()\nconf_int_df.columns = ['variable', 'ci_lower', 'ci_upper']\n\n# Merge these statistics into coef_df\ncoef_df = coef_df.merge(stderr_df, on='variable')\ncoef_df = coef_df.merge(tvalues_df, on='variable')\ncoef_df = coef_df.merge(pvalues_df, on='variable')\ncoef_df = coef_df.merge(conf_int_df, on='variable')\n\ncoef_df\ncoef_df.to_csv('regression results.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:42:39.608873Z","iopub.execute_input":"2024-07-02T22:42:39.609298Z","iopub.status.idle":"2024-07-02T22:42:39.71338Z","shell.execute_reply.started":"2024-07-02T22:42:39.609269Z","shell.execute_reply":"2024-07-02T22:42:39.712159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter for 'C(hour)' variables\nhour_coef_df = coef_df[coef_df['variable'].str.contains(r'C\\(hour\\)\\[T\\.\\d+\\]')]\n# Extract hour values from the variable names\nhour_coef_df['hour'] = hour_coef_df['variable'].str.extract(r'C\\(hour\\)\\[T\\.(\\d+)\\]', expand=False).astype(int)\n\nhour_coef_df","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:42:46.70128Z","iopub.execute_input":"2024-07-02T22:42:46.701719Z","iopub.status.idle":"2024-07-02T22:42:46.732783Z","shell.execute_reply.started":"2024-07-02T22:42:46.70169Z","shell.execute_reply":"2024-07-02T22:42:46.731136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualisation","metadata":{}},{"cell_type":"code","source":"# Create the plot\nfig_hour = go.Figure()\n\n# Add the main line for coefficients\nfig_hour.add_trace(go.Scatter(\n    x=hour_coef_df['hour'],\n    y=hour_coef_df['coefficient'],\n    mode='lines+markers',\n    name='Coefficient'\n))\n\n# Add the confidence interval shaded area\nfig_hour.add_trace(go.Scatter(\n    x=pd.concat([hour_coef_df['hour'], hour_coef_df['hour'][::-1]]),\n    y=pd.concat([hour_coef_df['ci_upper'], hour_coef_df['ci_lower'][::-1]]),\n    fill='toself',\n    fillcolor='rgba(0,100,80,0.2)',\n    line=dict(color='rgba(255,255,255,0)'),\n    hoverinfo=\"skip\",\n    showlegend=True,\n    name='95% CI'\n))\n\n# Add a horizontal line at zero\nfig_hour.add_shape(\n    type='line',\n    x0=0,\n    y0=0,\n    x1=1,\n    y1=0,\n    xref='paper',\n    yref='y',\n    line=dict(color='darkgray', width=2)\n)\n\n# Customize the layout\nfig_hour.update_layout(\n    title='Estimated Coefficients for C(hour) with 95% Confidence Interval',\n    xaxis_title='Hour',\n    yaxis_title='Coefficient',\n    template='plotly_white',\n    # Customize the layout to show all hour values on the x-axis\n    xaxis=dict(\n        tickmode='array',\n        tickvals=list(range(24)),\n        ticktext=[str(hour) for hour in range(24)]\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:43:07.121589Z","iopub.execute_input":"2024-07-02T22:43:07.122055Z","iopub.status.idle":"2024-07-02T22:43:08.048013Z","shell.execute_reply.started":"2024-07-02T22:43:07.12202Z","shell.execute_reply":"2024-07-02T22:43:08.046722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter for 'C(weekday)' variables\nwd_coef_df = coef_df[coef_df['variable'].str.contains(r'C\\(weekday\\)\\[T\\.\\w+\\]')]\nwd_coef_df\n# Extract weekdays from the variable names\nwd_coef_df['weekday'] = wd_coef_df['variable'].str.extract(r'C\\(weekday\\)\\[T\\.(\\w+)\\]', expand=False)\n\n# Sort the dataframes by 'weekday' for proper plotting\n# Mapping of day names to day numbers\nday_mapping = {\n    'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5, 'Saturday': 6,\n    'Sunday': 7\n}\n\n# Map month names to month numbers\nwd_coef_df['day_number'] = wd_coef_df['weekday'].map(day_mapping)\n\n# Sort by month number\nwd_coef_df = wd_coef_df.sort_values('day_number')\nwd_coef_df","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:44:06.34512Z","iopub.execute_input":"2024-07-02T22:44:06.345605Z","iopub.status.idle":"2024-07-02T22:44:06.388694Z","shell.execute_reply.started":"2024-07-02T22:44:06.345568Z","shell.execute_reply":"2024-07-02T22:44:06.387226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the plot for 'month' coefficients\nfig_wd = go.Figure()\n\n# Add the main line for 'month' coefficients\nfig_wd.add_trace(go.Scatter(\n    x=wd_coef_df['weekday'],\n    y=wd_coef_df['coefficient'],\n    mode='lines+markers',\n    name='Coefficient'\n))\n\n# Add error bars as a separate trace\nfig_wd.add_trace(go.Scatter(\n    x=wd_coef_df['weekday'],\n    y=wd_coef_df['coefficient'],\n    mode='markers',\n    name='95% CI',\n    error_y=dict(\n        type='data',\n        symmetric=False,\n        array=wd_coef_df['ci_upper'] - wd_coef_df['coefficient'],\n        arrayminus=wd_coef_df['coefficient'] - wd_coef_df['ci_lower'],\n        width=1.5,\n        thickness=1.5,\n        color='rgba(0,100,80,0.2)'\n    ),\n    marker=dict(color='rgba(0,100,80,0.2)')\n))\n\n# Add a horizontal line at zero\nfig_wd.add_shape(\n    type='line',\n    x0=0,\n    y0=0,\n    x1=1,\n    y1=0,\n    xref='paper',\n    yref='y',\n    line=dict(color='darkgray', width=2)\n)\n\n# Customize the layout\nfig_wd.update_layout(\n    title='Estimated Coefficients for C(weekday) with 95% Confidence Intervals',\n    xaxis_title='Weekdays',\n    yaxis_title='Coefficient',\n    template='plotly_white',\n    xaxis=dict(\n        tickmode='array',\n        tickvals=wd_coef_df['weekday'],\n        ticktext=wd_coef_df['weekday']\n    )\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:44:27.010674Z","iopub.execute_input":"2024-07-02T22:44:27.011599Z","iopub.status.idle":"2024-07-02T22:44:27.120545Z","shell.execute_reply.started":"2024-07-02T22:44:27.011558Z","shell.execute_reply":"2024-07-02T22:44:27.119132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the plot for 'month' coefficients\nfig_wd = go.Figure()\n\n# Add the main line for 'month' coefficients\nfig_wd.add_trace(go.Scatter(\n    x=wd_coef_df['weekday'],\n    y=wd_coef_df['coefficient'],\n    mode='lines+markers',\n    name='Coefficient'\n))\n\n# Add error bars as a separate trace\nfig_wd.add_trace(go.Scatter(\n    x=wd_coef_df['weekday'],\n    y=wd_coef_df['coefficient'],\n    mode='markers',\n    name='95% CI',\n    error_y=dict(\n        type='data',\n        symmetric=False,\n        array=wd_coef_df['ci_upper'] - wd_coef_df['coefficient'],\n        arrayminus=wd_coef_df['coefficient'] - wd_coef_df['ci_lower'],\n        width=1.5,\n        thickness=1.5,\n        color='rgba(0,100,80,0.2)'\n    ),\n    marker=dict(color='rgba(0,100,80,0.2)')\n))\n\n# Add a horizontal line at zero\nfig_wd.add_shape(\n    type='line',\n    x0=0,\n    y0=0,\n    x1=1,\n    y1=0,\n    xref='paper',\n    yref='y',\n    line=dict(color='darkgray', width=2)\n)\n\n# Customize the layout\nfig_wd.update_layout(\n    title='Estimated Coefficients for C(weekday) with 95% Confidence Intervals',\n    xaxis_title='Weekdays',\n    yaxis_title='Coefficient',\n    template='plotly_white',\n    xaxis=dict(\n        tickmode='array',\n        tickvals=wd_coef_df['weekday'],\n        ticktext=wd_coef_df['weekday']\n    )\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:45:47.936294Z","iopub.execute_input":"2024-07-02T22:45:47.936825Z","iopub.status.idle":"2024-07-02T22:45:48.018179Z","shell.execute_reply.started":"2024-07-02T22:45:47.936787Z","shell.execute_reply":"2024-07-02T22:45:48.016831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter for 'C(month)' variables\nmonth_coef_df = coef_df[coef_df['variable'].str.contains(r'C\\(month\\)\\[T\\.\\w+\\]')]\n# Extract month values from the variable names\nmonth_coef_df['month'] = month_coef_df['variable'].str.extract(r'C\\(month\\)\\[T\\.(\\w+)\\]', expand=False)\nmonth_coef_df\n\n# Sort the dataframes by 'month' for proper plotting\n# Mapping of month names to month numbers\nmonth_mapping = {\n    'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June': 6,\n    'July': 7, 'August': 8, 'September': 9, 'October': 10, 'November': 11, 'December': 12\n}\n\n# Map month names to month numbers\nmonth_coef_df['month_number'] = month_coef_df['month'].map(month_mapping)\n\n# Sort by month number\nmonth_coef_df = month_coef_df.sort_values('month_number')\n\nmonth_coef_df","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:46:02.736938Z","iopub.execute_input":"2024-07-02T22:46:02.737377Z","iopub.status.idle":"2024-07-02T22:46:02.786649Z","shell.execute_reply.started":"2024-07-02T22:46:02.737344Z","shell.execute_reply":"2024-07-02T22:46:02.785542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the plot for 'month' coefficients\nfig_month = go.Figure()\n\n# Add the main line for 'month' coefficients\nfig_month.add_trace(go.Scatter(\n    x=month_coef_df['month'],\n    y=month_coef_df['coefficient'],\n    mode='lines+markers',\n    name='Coefficient'\n))\n\n# Add error bars as a separate trace\nfig_month.add_trace(go.Scatter(\n    x=month_coef_df['month'],\n    y=month_coef_df['coefficient'],\n    mode='markers',\n    name='95% CI',\n    error_y=dict(\n        type='data',\n        symmetric=False,\n        array=month_coef_df['ci_upper'] - month_coef_df['coefficient'],\n        arrayminus=month_coef_df['coefficient'] - month_coef_df['ci_lower'],\n        width=1.5,\n        thickness=1.5,\n        color='rgba(0,100,80,0.2)'\n    ),\n    marker=dict(color='rgba(0,100,80,0.2)')\n))\n\n# Add a horizontal line at zero\nfig_month.add_shape(\n    type='line',\n    x0=0,\n    y0=0,\n    x1=1,\n    y1=0,\n    xref='paper',\n    yref='y',\n    line=dict(color='darkgray', width=2)\n)\n\n# Customize the layout\nfig_month.update_layout(\n    title='Estimated Coefficients for C(month) with 95% Confidence Intervals',\n    xaxis_title='Months',\n    yaxis_title='Coefficient',\n    template='plotly_white',\n    xaxis=dict(\n        tickmode='array',\n        tickvals=month_coef_df['month'],\n        ticktext=month_coef_df['month']\n    )\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:46:19.780278Z","iopub.execute_input":"2024-07-02T22:46:19.780752Z","iopub.status.idle":"2024-07-02T22:46:19.864936Z","shell.execute_reply.started":"2024-07-02T22:46:19.780716Z","shell.execute_reply":"2024-07-02T22:46:19.863507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter for 'C(year)' variables\nyr_coef_df = coef_df[coef_df['variable'].str.contains(r'C\\(year\\)\\[T\\.\\d+\\]')]\n\n# Extract hour values from the variable names\nyr_coef_df['year'] = yr_coef_df['variable'].str.extract(r'C\\(year\\)\\[T\\.(\\d+)\\]', expand=False).astype(int)\nyr_coef_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the plot for 'year' coefficients\nfig_yr = go.Figure()\n\n# Add the main line for 'year' coefficients\nfig_yr.add_trace(go.Scatter(\n    x=yr_coef_df['year'],\n    y=yr_coef_df['coefficient'],\n    mode='lines+markers',\n    name='Coefficient'\n))\n\n# Add error bars as a separate trace\nfig_yr.add_trace(go.Scatter(\n    x=yr_coef_df['year'],\n    y=yr_coef_df['coefficient'],\n    mode='markers',\n    name='95% CI',\n    error_y=dict(\n        type='data',\n        symmetric=False,\n        array=yr_coef_df['ci_upper'] - yr_coef_df['coefficient'],\n        arrayminus=yr_coef_df['coefficient'] - yr_coef_df['ci_lower'],\n        width=1.5,\n        thickness=1.5,\n        color='rgba(0,100,80,0.2)'\n    ),\n    marker=dict(color='rgba(0,100,80,0.2)')\n))\n\n# Add a horizontal line at zero\nfig_yr.add_shape(\n    type='line',\n    x0=0,\n    y0=0,\n    x1=1,\n    y1=0,\n    xref='paper',\n    yref='y',\n    line=dict(color='darkgray', width=2)\n)\n\n# Customize the layout\nfig_yr.update_layout(\n    title='Estimated Coefficients for C(year) with 95% Confidence Intervals',\n    xaxis_title='Years',\n    yaxis_title='Coefficient',\n    template='plotly_white',\n    xaxis=dict(\n        tickmode='array',\n        tickvals=yr_coef_df['year'],\n        ticktext=yr_coef_df['year']\n    )\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# **Q.5**\n   * **5.1: Suggested locations for the deployment of drivers**\n        \n   * **5.2: Locations we Start**\n   \n   * **5.3: Calculation of the busiest destinations for the proposed destinations**\n        * 5.3.1: The busiest routes at 00:00 to 6:00 in the morning\n        * 5.3.2: The busiest routes at 06:00 to 12:00 in the morning\n        * 5.3.3: The busiest routes at 12:00 to 18:00 in the morning\n        * 5.3.4: The busiest routes at 18:00 to 23:59 in the morning\n\n   * **5.4: Graph of the number of repetitions of each busy route in the 4 desired time periods**\n   \n   * **5.5: Graph of the total number of trips in the total hours of the day for each route**\n \n   \n   ","metadata":{}},{"cell_type":"markdown","source":"## 5.1: Suggested locations for the deployment of drivers","metadata":{}},{"cell_type":"code","source":"conn.sql('''\nCREATE OR REPLACE VIEW new_tlctrip AS\nSELECT \n     PUZone as start_zone\n    ,PUlocationID AS start_location_id \n    ,count(*) AS number_of_rides\n    ,sum(total_amount) AS sum_of_price\n    ,percentile_disc(0.50) WITHIN GROUP (ORDER BY total_amount) AS price_quartile_50\n    ,number_of_rides * price_quartile_50 AS metric\n    ,sum_of_price/(SELECT SUM(total_amount) FROM tlctrip WHERE unified_pickup_datetime BETWEEN '2023-04-01 00:00:00' AND '2024-03-31 00:00:00')*100 AS market_cap_percentage\nFROM tlctrip\nWHERE \n    unified_pickup_datetime BETWEEN '2023-04-01 00:00:00' AND '2024-03-31 00:00:00'  \nGROUP BY 1,2\nORDER BY 6 desc\n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:50:29.736625Z","iopub.execute_input":"2024-07-02T22:50:29.737096Z","iopub.status.idle":"2024-07-02T22:50:29.872802Z","shell.execute_reply.started":"2024-07-02T22:50:29.737059Z","shell.execute_reply":"2024-07-02T22:50:29.871442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2: Location we start","metadata":{}},{"cell_type":"markdown","source":"We have started top 5 locations which have 30 percent of trips","metadata":{}},{"cell_type":"code","source":"df51 = conn.sql('''\nSELECT \n    *\n    ,SUM(market_cap_percentage) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_market_cap_percentage\nFROM new_tlctrip\nLIMIT 5\n''').to_df()\ndf51","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:55:39.677225Z","iopub.execute_input":"2024-07-02T22:55:39.678466Z","iopub.status.idle":"2024-07-02T22:55:49.450808Z","shell.execute_reply.started":"2024-07-02T22:55:39.678401Z","shell.execute_reply":"2024-07-02T22:55:49.449236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.3: Calculation of the busiest destinations for the proposed destinations","metadata":{}},{"cell_type":"code","source":"df52 = conn.sql('''\nWITH ranked_rides AS (\n  SELECT \n    PUZone AS start_location,\n    DOZone AS end_location,\n    COUNT(*) AS number_of_rides,\n    DENSE_RANK() OVER (PARTITION BY start_location ORDER BY number_of_rides DESC) AS rank\n  FROM \n    tlctrip\n  WHERE\n    unified_pickup_datetime BETWEEN '2023-04-01 00:00:00' AND '2024-03-31 00:00:00' \n    AND start_location in ('JFK Airport','LaGuardia Airport','Midtown Center','Upper East Side South', 'Upper East Side North')\n    GROUP BY 1,2\n)\nSELECT *\nFROM ranked_rides\nWHERE rank=1\nORDER BY start_location;''').to_df()\ndf52","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:11:38.555779Z","iopub.execute_input":"2024-07-02T23:11:38.556321Z","iopub.status.idle":"2024-07-02T23:11:41.0222Z","shell.execute_reply.started":"2024-07-02T23:11:38.556283Z","shell.execute_reply":"2024-07-02T23:11:41.02093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.3.1: The busiest routes at 00:00 to 6:00 in the morning","metadata":{}},{"cell_type":"code","source":"df52 = conn.sql('''\nWITH ranked_rides AS (\n  SELECT \n    PUZone AS start_location,\n    DOZone AS end_location,\n    COUNT(*) AS number_of_rides,\n    DENSE_RANK() OVER (PARTITION BY start_location ORDER BY number_of_rides DESC) AS rank\n  FROM \n    tlctrip\n  WHERE\n    unified_pickup_datetime BETWEEN '2023-04-01 00:00:00' AND '2024-03-31 00:00:00' \n    AND start_location in ('JFK Airport','LaGuardia Airport','Midtown Center','Upper East Side South', 'Upper East Side North')\n    AND HOUR(unified_pickup_datetime) between 0 and 5\n    GROUP BY 1,2\n)\nSELECT *\nFROM ranked_rides\nWHERE rank=1\nORDER BY start_location;''').to_df()\ndf52","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.3.2: The busiest routes at 06:00 to 12:00 in the morning","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df53 = conn.sql('''\nWITH ranked_rides AS (\n  SELECT \n    PUZone AS start_location,\n    DOZone AS end_location,\n    COUNT(*) AS number_of_rides,\n    DENSE_RANK() OVER (PARTITION BY start_location ORDER BY number_of_rides DESC) AS rank\n  FROM \n    tlctrip\n  WHERE\n    unified_pickup_datetime BETWEEN '2023-04-01 00:00:00' AND '2024-03-31 00:00:00' \n    AND start_location in ('JFK Airport','LaGuardia Airport','Midtown Center','Upper East Side South', 'Upper East Side North')\n    AND HOUR(unified_pickup_datetime) between 6 and 11\n    GROUP BY 1,2\n)\nSELECT *\nFROM ranked_rides\nWHERE rank=1\nORDER BY start_location;''').to_df()\ndf53","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.3.3: The busiest routes at 12:00 to 18:00 in the morning","metadata":{}},{"cell_type":"code","source":"df54 = conn.sql('''\nWITH ranked_rides AS (\n  SELECT \n    PUZone AS start_location,\n    DOZone AS end_location,\n    COUNT(*) AS number_of_rides,\n    DENSE_RANK() OVER (PARTITION BY start_location ORDER BY number_of_rides DESC) AS rank\n  FROM \n    tlctrip\n  WHERE\n    unified_pickup_datetime BETWEEN '2023-04-01 00:00:00' AND '2024-03-31 00:00:00' \n    AND start_location in ('JFK Airport','LaGuardia Airport','Midtown Center','Upper East Side South', 'Upper East Side North')\n    AND HOUR(unified_pickup_datetime) between 12 and 17\n    GROUP BY 1,2\n)\nSELECT *\nFROM ranked_rides\nWHERE rank=1\nORDER BY start_location;''').to_df()\ndf54","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.3.4: The busiest routes at 18:00 to 23:59 in the morning","metadata":{}},{"cell_type":"code","source":"df55 = conn.sql('''\nWITH ranked_rides AS (\n  SELECT \n    PUZone AS start_location,\n    DOZone AS end_location,\n    COUNT(*) AS number_of_rides,\n    DENSE_RANK() OVER (PARTITION BY start_location ORDER BY number_of_rides DESC) AS rank\n  FROM \n    tlctrip\n  WHERE\n    unified_pickup_datetime BETWEEN '2023-04-01 00:00:00' AND '2024-03-31 00:00:00' \n    AND start_location in ('JFK Airport','LaGuardia Airport','Midtown Center','Upper East Side South', 'Upper East Side North')\n    AND HOUR(unified_pickup_datetime) between 18 and 23\n    GROUP BY 1,2\n)\nSELECT *\nFROM ranked_rides\nWHERE rank=1\nORDER BY start_location;''').to_df()\ndf55","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a list containing all your DataFrames\ndf_list = [df52, df53, df54, df55]\n\n# Concatenate the DataFrames vertically (adding rows)\ndf_combined = pd.concat(df_list)\n\n# Print the result\ndf_combined","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df56 = conn.sql('''\nSELECT \n     start_location\n    ,end_location\n    ,COUNT(*) AS number_of_occurance\n    ,SUM(number_of_rides) AS Total_number_of_rides\nFROM df_combined\nGROUP BY 1,2\n''').to_df()\n\ndf56['path'] = df56['start_location'] + ' - ' + df56['end_location']\ndf56","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.4: Graph of the number of repetitions of each busy route in the 4 desired time periods","metadata":{}},{"cell_type":"code","source":"df_sorted = df56.sort_values(by='number_of_occurance', ascending=True)\n\n# Create the horizontal bar chart using px.bar\nfig = px.bar(\n    df_sorted,  # Data source (sorted DataFrame)\n    x='number_of_occurance',  # x-axis (market_cap_percentage)\n    y='path',  # y-axis (zone)\n    title='Occurance of Path in different time periods',\n    orientation='h',  # Horizontal orientation\n    #color='path',  # Color based on market_cap_percentage\n    text=df_sorted['number_of_occurance'].round(1),  # Display values as text on bars\n    labels={  # Customize axis labels\n        'path': 'Path Name',\n        'number_of_occurance': 'Occurance'\n    }\n)\n\n# Display the chart\nfig.show(renderer='iframe')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.5: Graph of the total number of trips in the total hours of the day for each route","metadata":{}},{"cell_type":"code","source":"df_sorted = df56.sort_values(by='Total_number_of_rides', ascending=True)\n\n# Create the horizontal bar chart using px.bar\nfig = px.bar(\n    df_sorted,  # Data source (sorted DataFrame)\n    x='Total_number_of_rides',  # x-axis (market_cap_percentage)\n    y='path',  # y-axis (zone)\n    title='Total Number of rides in 1 Recent year For Each Path ',\n    orientation='h',  # Horizontal orientation\n    #color='path',  # Color based on market_cap_percentage\n    text=df_sorted['Total_number_of_rides'].round(1),  # Display values as text on bars\n    labels={  # Customize axis labels\n        'path': 'Path Name',\n        'Total_number_of_rides':'Number Of Rides '\n    }\n)\n\n# Display the chart\nfig.show(renderer='iframe')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Q.6**\n   * **6.1: Creating View and Dataframe For Analyzing Data**\n        \n   * **6.2: Calculating Correlation and Statistics**\n   \n   * **6.3: Visualisation**\n   \n\n \n ","metadata":{}},{"cell_type":"markdown","source":"## 6.1: Creating View and Dataframe For Analyzing Data","metadata":{}},{"cell_type":"code","source":"conn.sql('''\nCREATE OR REPLACE VIEW num6_prepration AS\nSELECT\n    unified_pickup_datetime AS Pickup_Datetime,\n    trip_distance * 1.60934 AS Distance_KM,\n    total_amount AS Price,\n    (EXTRACT(EPOCH FROM unified_dropoff_datetime) - EXTRACT(EPOCH FROM unified_pickup_datetime))/60 AS Duration_Min,\n    HOUR(unified_pickup_datetime) AS Time_Hour,\n    CASE\n        WHEN Time_Hour BETWEEN 0 AND 3 THEN '1'\n        WHEN Time_Hour BETWEEN 4 AND 7 THEN '2'\n        WHEN Time_Hour BETWEEN 8 AND 11 THEN '3'\n        WHEN Time_Hour BETWEEN 12 AND 15 THEN '4'\n        WHEN Time_Hour BETWEEN 16 AND 19 THEN '5'\n        WHEN Time_Hour BETWEEN 20 AND 23 THEN '6'\n        ELSE '7'\n    END AS Time_Bins,\n    CASE\n    WHEN trip_distance * 1.60934 < 2 THEN 1\n    WHEN trip_distance * 1.60934 < 4 THEN 2\n    WHEN trip_distance * 1.60934 < 6 THEN 3\n    WHEN trip_distance * 1.60934 < 8 THEN 4\n    WHEN trip_distance * 1.60934 < 10 THEN 5\n    WHEN trip_distance * 1.60934 < 12 THEN 6\n    WHEN trip_distance * 1.60934 < 14 THEN 7\n    WHEN trip_distance * 1.60934 < 16 THEN 8\n    WHEN trip_distance * 1.60934 < 18 THEN 9\n    WHEN trip_distance * 1.60934 < 20 THEN 10\n    WHEN trip_distance * 1.60934 < 22 THEN 11\n    WHEN trip_distance * 1.60934 < 24 THEN 12\n    WHEN trip_distance * 1.60934 < 26 THEN 13\n    WHEN trip_distance * 1.60934 < 28 THEN 14\n    WHEN trip_distance * 1.60934 < 30 THEN 15\n    WHEN trip_distance * 1.60934 < 32 THEN 16\n    WHEN trip_distance * 1.60934 < 34 THEN 17\n    WHEN trip_distance * 1.60934 < 36 THEN 18\n    WHEN trip_distance * 1.60934 < 38 THEN 19\n    WHEN trip_distance * 1.60934 < 40 THEN 20\n    WHEN trip_distance * 1.60934 < 42 THEN 21\n    WHEN trip_distance * 1.60934 < 44 THEN 22\n    WHEN trip_distance * 1.60934 < 46 THEN 23\n    WHEN trip_distance * 1.60934 < 48 THEN 24\n    WHEN trip_distance * 1.60934 < 50 THEN 25\n    ELSE 26\n    END AS Distance_Bin,\n    (trip_distance * 1.60934) / ((EXTRACT(EPOCH FROM unified_dropoff_datetime) - EXTRACT(EPOCH FROM unified_pickup_datetime))/3600) AS Speed,\n    CASE\n    WHEN (trip_distance * 1.60934) / ((EXTRACT(EPOCH FROM unified_dropoff_datetime) - EXTRACT(EPOCH FROM unified_pickup_datetime))/3600) < 10 THEN 1\n    WHEN (trip_distance * 1.60934) / ((EXTRACT(EPOCH FROM unified_dropoff_datetime) - EXTRACT(EPOCH FROM unified_pickup_datetime))/3600) < 20 THEN 2\n    WHEN (trip_distance * 1.60934) / ((EXTRACT(EPOCH FROM unified_dropoff_datetime) - EXTRACT(EPOCH FROM unified_pickup_datetime))/3600) < 30 THEN 3\n    WHEN (trip_distance * 1.60934) / ((EXTRACT(EPOCH FROM unified_dropoff_datetime) - EXTRACT(EPOCH FROM unified_pickup_datetime))/3600) < 40 THEN 4\n    WHEN (trip_distance * 1.60934) / ((EXTRACT(EPOCH FROM unified_dropoff_datetime) - EXTRACT(EPOCH FROM unified_pickup_datetime))/3600) < 50 THEN 5\n    WHEN (trip_distance * 1.60934) / ((EXTRACT(EPOCH FROM unified_dropoff_datetime) - EXTRACT(EPOCH FROM unified_pickup_datetime))/3600) < 60 THEN 6\n    WHEN (trip_distance * 1.60934) / ((EXTRACT(EPOCH FROM unified_dropoff_datetime) - EXTRACT(EPOCH FROM unified_pickup_datetime))/3600) < 70 THEN 7\n    ELSE 8\n    END AS Speed_Bin\nFROM\n    tlctrip\nWHERE\nPickup_Datetime > '2023-03-01';\n''')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conn.sql('''CREATE OR REPLACE VIEW df AS\nSELECT \n    Pickup_Datetime,\n    Distance_KM,\n    Time_Hour,\n    Speed,\n    Speed_Bin,\n    CAST(Time_Bins AS INT) AS Time_Bin,\n    CAST(Speed_Bin AS INT) AS Speed_Bin,\n    CAST(Distance_Bin AS INT) AS Distance_Bin,\n    Price,\n    EXTRACT(EPOCH FROM Pickup_Datetime) AS Pickup_Datetime_Epoch,\nFROM num6_prepration''')\n# Analyze correlation and statistical results\n\ndf51 = conn.sql('''SELECT \n    Pickup_Datetime,\n    Distance_Bin,\n    Speed_Bin,\n    Time_Bin,\n    Price,\n    Pickup_Datetime_Epoch,\nFROM df''').to_df()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.2: Calculating Correlation and Statistics","metadata":{}},{"cell_type":"code","source":"correlation_results = []\n\nfor ( dist_bin, time_bin, speed_bin), group in df51.groupby(['Distance_Bin', 'Time_Bin', 'Speed_Bin']):\n    correlation = group['Pickup_Datetime_Epoch'].corr(group['Price'])\n    r_squared = correlation ** 2  # Calculate the coefficient of determination\n    correlation_results.append({\n        'Distance_Bin': dist_bin,\n        'Time_Bin': time_bin,\n        'Speed_Bin': speed_bin,\n        'Correlation': correlation,\n        'Coefficient': r_squared  # Add R_squared value to the results\n    })\n\n# Create a DataFrame from the correlation results\ncorrelation_df = pd.DataFrame(correlation_results)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conn.sql('''CREATE OR REPLACE VIEW statistics AS\nSELECT\n    Distance_Bin,\n    Time_Bin,\n    Speed_Bin,\n    Count (*) AS Count,\n    AVG(Price) AS Mean_Price,\n    MEDIAN(Price) AS Median_Price,\n    VAR_SAMP(Price) AS Variance_Price,\n    STDDEV(Price) AS Stddev_Price,\n    MIN(Price) AS Min_Price,\n    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY Price) AS Q1_Price,\n    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY Price) AS Q2_Price,\n    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY Price) AS Q3_Price,\n    MAX(Price) AS Max_Price,\nFROM\n    df\nGROUP BY\n    Distance_Bin,\n    Time_Bin,\n    Speed_Bin,\nHAVING\n    COUNT(*) >= 20 \nORDER BY\n    Distance_Bin,\n    Time_Bin,\n    Speed_Bin;\n    ''')\nstatistics = conn.sql('''select * from statistics ''').to_df()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a unique identifier for each combination of , Distance_Bin, Time_Bins, and Speed_Bin in the statistics DataFrame\nstatistics['unique_id'] = statistics['Distance_Bin'].astype(str) + '_' + \\\n                          statistics['Time_Bin'].astype(str) + '_' + \\\n                          statistics['Speed_Bin'].astype(str)\n\n# Create a unique identifier for each combination of PULocationID, Distance_Bin, Time_Bins, and Speed_Bin in the correlation_df DataFrame\ncorrelation_df['unique_id'] = correlation_df['Distance_Bin'].astype(str) + '_' + \\\n                              correlation_df['Time_Bin'].astype(str) + '_' + \\\n                              correlation_df['Speed_Bin'].astype(str)\n\n# Merge the dataframes based on the unique identifier\nmerged_df = pd.merge(correlation_df, statistics, on='unique_id', how='inner')\n\n# Drop the 'unique_id' column if not needed\nmerged_df.drop(columns=['unique_id'], inplace=True)\n\n# Now merged_df contains the combined information from both dataframes\nmerged_df.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.3: Visualisation","metadata":{}},{"cell_type":"code","source":"columns_to_plot = ['Correlation', 'Coefficient', 'Count', \n                   'Mean_Price', 'Median_Price', \n                   'Variance_Price', 'Stddev_Price']\n\n# Determine the number of rows and columns for the subplots (here, 4 rows and 2 columns are used)\nfig = make_subplots(rows=4, cols=2, \n                    subplot_titles=columns_to_plot,\n                    shared_yaxes=False)\n\n# Loop to add each histogram to its respective subplot position\nfor i, column in enumerate(columns_to_plot, 1):\n    row = (i - 1) // 2 + 1  # Calculate the row number for the subplot\n    col = (i - 1) % 2 + 1   # Calculate the column number for the subplot\n    fig.add_trace(\n        go.Histogram(x=merged_df[column], name=f'Distribution of {column}'),\n        row=row, col=col\n    )\n\n# Update the visual layout settings of the figure\nfig.update_layout(\n    title_text=\"Distribution of Various Columns\",\n    height=1200,  # Set the height as desired\n    width=1000,   # Set the width as desired\n    showlegend=False  # Do not show the legend box\n)\n\n# Show the figure\nfig.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assume df51 is already defined and contains the necessary data\n\n# Mapping dictionaries for labels\ntime_bin_labels = {\n    1: '0-4 AM', 2: '4-8 AM', 3: '8-12 PM',\n    4: '12-4 PM', 5: '4-8 PM', 6: '8-12 AM'\n}\n\nspeed_bin_labels = {\n    1: '0 to 10 KM/H', 2: '10 to 20 KM/H', 3: '20 to 30 KM/H', 4: '30 to 40 KM/H', \n    5: '40 to 50 KM/H', 6: '50 to 60 KM/H', 7: '60 to 70 KM/H', 8: 'More than 70 KM/H'\n}\n\ndistance_bin_labels = {\n    1: '2 KM', 2: '4 KM', 3: '6 KM', 4: '8 KM',\n    5: '10 KM', 6: '12 KM', 7: '14 KM', 8: '16 KM',\n    9: '18 KM', 10: '20 KM', 11: '22 KM', 12: '24 KM',\n    13: '26 KM', 14: '28 KM', 15: '30 KM', 16: '32 KM',\n    17: '34 KM', 18: '36 KM', 19: '38 KM', 20: '40 KM',\n    21: '42 KM', 22: '44 KM', 23: '46 KM', 24: '48 KM',\n    25: '50 KM', 26: 'More than 50 KM'\n}\n\nspeed_bin_range = range(1, 9)  # Speed_Bin range: 1 to 8\ndistance_bin_range = range(1, 27)  # Distance_Bin range: 1 to 26\ntime_bin_range = range(1, 7)  # Time_Bin range: 1 to 6\n\n# Iterate through all combinations of Distance_Bin, Time_Bin, and Speed_Bin\nfor distance_bin in distance_bin_range:\n    for time_bin in time_bin_range:\n        for speed_bin in speed_bin_range:\n            # Filter the data for the current combination\n            filtered_data = df51[\n                (df51['Distance_Bin'] == distance_bin) &\n                (df51['Time_Bin'] == time_bin) &\n                (df51['Speed_Bin'] == speed_bin)\n            ].copy()\n\n            # Check if there is data for this combination, if not, skip to the next\n            if filtered_data.empty:\n                continue\n\n            # Sort the data by Pickup_Datetime\n            filtered_data.sort_values(by='Pickup_Datetime', inplace=True)\n\n            # Calculate the count of the filtered data\n            count = len(filtered_data)\n\n            # Create the scatter plot with trendline using Plotly Express\n            fig = px.scatter(filtered_data, x='Pickup_Datetime', y='Price',\n                             title=f'Price Change Over Time - Distance: {distance_bin_labels[distance_bin]}, '\n                                   f'Time: {time_bin_labels[time_bin]}, Speed: {speed_bin_labels[speed_bin]} - Count: {count}',\n                             labels={'Pickup_Datetime': 'Pickup Datetime', 'Price': 'Price'},\n                             trendline=\"ols\")  # Adding trendline with method=\"ols\" (ordinary least squares)\n\n            # Show the plot\n            fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Q.7**\n   * **7.1: Creating View and Dataframe For Analyzing Data**\n        \n   * **7.2: Finding Same Path**\n   \n   * **7.3: Save the result_df to a CSV file**\n   \n\n \n ","metadata":{}},{"cell_type":"markdown","source":"## 7.1: Creating View and Dataframe For Analyzing Data\n\n","metadata":{}},{"cell_type":"code","source":"# Create a view table of our dataset.\n\n\nconn.sql('''\nCREATE OR REPLACE VIEW tlctrip AS\nSELECT \n    *,\n    CASE \n        WHEN cap_type = 'green' THEN lpep_pickup_datetime\n        WHEN cap_type = 'yellow' THEN tpep_pickup_datetime\n        ELSE NULL\n    END AS unified_pickup_datetime,\n    CASE \n        WHEN cap_type = 'green' THEN lpep_dropoff_datetime\n        WHEN cap_type = 'yellow' THEN tpep_dropoff_datetime\n        ELSE NULL\n    END AS unified_dropoff_datetime\nFROM \n    read_parquet('/kaggle/input/tlc-trip-2020-01-to-2024-03/trips/*/*/*.parquet', hive_partitioning = true)\nWHERE\n    (\n        unified_pickup_datetime < '2024-04-01' AND \n        unified_pickup_datetime >= '2020-01-01'\n    ) AND\n    (\n        (fare_amount >= 0) AND\n        (total_amount >= 0) AND\n        ((extra >= 0) OR (extra IS NULL)) AND\n        ((mta_tax >= 0) OR (mta_tax IS NULL)) AND\n        ((tip_amount >= 0) OR (tip_amount IS NULL)) AND\n        ((tolls_amount >= 0) OR (tolls_amount IS NULL)) AND\n        ((ehail_fee >= 0) OR (ehail_fee IS NULL)) AND\n        ((improvement_surcharge >= 0) OR (improvement_surcharge IS NULL)) AND\n        ((congestion_surcharge >= 0) OR (congestion_surcharge IS NULL)) AND\n        ((airport_fee >= 0) OR (airport_fee IS NULL))\n    )\n''')\n\n# Create a view table of NewYork taxizone dataset.\n\nconn.sql('''\nCREATE OR REPLACE VIEW taxizone AS \nSELECT * FROM 'https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv'\n''')\n\n# Get a view of our tlctrip view.\n\nconn.sql('''\nSELECT * FROM tlctrip LIMIT 2\n''').df()\n\n# Get a view of our taxizone view.\n\nconn.sql('''\nSELECT * FROM taxizone LIMIT 2\n''').df()\n\n# Get a look on fields data types.\n\nconn.sql('''\nDESCRIBE tlctrip\n''').df()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:22:02.732631Z","iopub.execute_input":"2024-07-02T23:22:02.733696Z","iopub.status.idle":"2024-07-02T23:22:05.041208Z","shell.execute_reply.started":"2024-07-02T23:22:02.733654Z","shell.execute_reply":"2024-07-02T23:22:05.03998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conn.sql('''\n-- Get only the column names for the taxizone view\nSELECT column_name\nFROM information_schema.columns\nWHERE table_name = 'taxizone';\n\n''')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:22:14.951137Z","iopub.execute_input":"2024-07-02T23:22:14.95161Z","iopub.status.idle":"2024-07-02T23:22:14.983074Z","shell.execute_reply.started":"2024-07-02T23:22:14.951573Z","shell.execute_reply":"2024-07-02T23:22:14.981961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.2: Finding Same Paths","metadata":{}},{"cell_type":"code","source":"# SQL query to count trips with the same start and end locations for the year 2023\nquery = '''\nSELECT \n    PULocationID, \n    DOLocationID, \n    COUNT(*) AS trip_count\nFROM \n    tlctrip\nWHERE\n    unified_pickup_datetime BETWEEN '2023-04-01' AND '2024-03-31'\nGROUP BY \n    PULocationID, \n    DOLocationID\nORDER BY \n    trip_count DESC\n'''\n\n# Execute the query and fetch the result into a Pandas DataFrame\nSamePath = conn.sql(query).df()\n\nSamePath","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:25:58.859471Z","iopub.execute_input":"2024-07-02T23:25:58.859888Z","iopub.status.idle":"2024-07-02T23:26:22.013404Z","shell.execute_reply.started":"2024-07-02T23:25:58.859858Z","shell.execute_reply":"2024-07-02T23:26:22.012237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 1: Extract the hour of day and count trips\nquery = '''\nSELECT \n    PUZone, \n    DOZone,\n    EXTRACT(HOUR FROM unified_pickup_datetime) AS hour_of_day,\n    COUNT(*) AS trip_count\nFROM \n    tlctrip\nWHERE\n    unified_pickup_datetime BETWEEN '2023-04-01' AND '2024-03-31'\nGROUP BY \n    PUZone, \n    DOZone,\n    hour_of_day\nORDER BY \n    PUZone, \n    DOZone,\n    hour_of_day\n'''\n\n# Execute the query and fetch the result into a Pandas DataFrame\ndf_hourly_distribution = conn.sql(query).df()\n\n# Step 2: Create the pivot table\npivot_df = df_hourly_distribution.pivot_table(\n    index=['PUZone', 'DOZone'], \n    columns='hour_of_day', \n    values='trip_count', \n    fill_value=0\n)\n\n# Step 3: Calculate the average trip_count for each hour\navg_trip_count_per_hour = pivot_df.mean()\n\n# Step 4: Filter rows where all trip_counts are above average for each hour\nabove_average_paths = pivot_df.apply(lambda row: all(row > avg_trip_count_per_hour[row.index]), axis=1)\n\nfiltered_df = pivot_df[above_average_paths]\n\n# Step 5: Calculate the total trip_count for each row (PULocationID, DOLocationID)\nfiltered_df['total_trip_count'] = filtered_df.sum(axis=1)\n\n# Step 6: Get the top 5 rows based on total_trip_count\ntop_5_rows = filtered_df.sort_values(by='total_trip_count', ascending=False).head(10)\n\n# Display the top 5 rows\ntop_5_rows.reset_index()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:27:36.753137Z","iopub.execute_input":"2024-07-02T23:27:36.754594Z","iopub.status.idle":"2024-07-02T23:27:51.443843Z","shell.execute_reply.started":"2024-07-02T23:27:36.754552Z","shell.execute_reply":"2024-07-02T23:27:51.442631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.3: Save the result_df to a CSV file","metadata":{}},{"cell_type":"code","source":"# Save the result_df to a CSV file\ntop_5_rows.to_csv('path_distribution.csv', index=False)\n\n# Optionally, display the path to the saved file\nprint('CSV file saved as path_distribution.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:28:31.036798Z","iopub.execute_input":"2024-07-02T23:28:31.037291Z","iopub.status.idle":"2024-07-02T23:28:31.060854Z","shell.execute_reply.started":"2024-07-02T23:28:31.037252Z","shell.execute_reply":"2024-07-02T23:28:31.059716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Q.8**\n        \n   * **8.1: Finding Top 1%**\n   \n   * **8.2: Distribution of Counts by Hour**\n   \n   * **8.3: Distribution of Counts by Weekdays**\n   \n   * **8.4: Distribution of Counts by Weather Condition**\n   \n   * **8.5: Calculate Correlations**\n   \n\n \n ","metadata":{}},{"cell_type":"markdown","source":"## 8.1: Finding Top 1% ","metadata":{}},{"cell_type":"code","source":"# Fetch all results\nTOP = conn.sql('''\nselect \n    year(unified_pickup_datetime) as year,\n    month(unified_pickup_datetime) as month,\n    day(unified_pickup_datetime) as day,\n    hour(unified_pickup_datetime) as hour,\n    count(*) as count\nfrom tlctrip\ngroup by 1, 2, 3, 4\norder by count desc\n''').to_df()\n\nprint (\"length TOP is : \",len(TOP))\nTOP1=TOP.head(372)\nprint (\"length TOP1 is : \",len(TOP1))\nTOP1\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:52:08.959839Z","iopub.execute_input":"2024-07-03T05:52:08.960287Z","iopub.status.idle":"2024-07-03T05:52:16.37004Z","shell.execute_reply.started":"2024-07-03T05:52:08.960254Z","shell.execute_reply":"2024-07-03T05:52:16.368904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.2: Distribution of Counts by Hour","metadata":{}},{"cell_type":"code","source":"fig = px.histogram(TOP1, x='hour', nbins=12, title='Distribution of Counts by Hour', text_auto=True)\n\n# Update layout to show the data labels more clearly\nfig.update_traces(textposition='outside', textfont_size=12)\nfig.show(renderer='iframe')","metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:54:20.941938Z","iopub.execute_input":"2024-07-03T05:54:20.942331Z","iopub.status.idle":"2024-07-03T05:54:23.199872Z","shell.execute_reply.started":"2024-07-03T05:54:20.942301Z","shell.execute_reply":"2024-07-03T05:54:23.198602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TOP1['date'] = pd.to_datetime(TOP1[['year', 'month', 'day']])\n\n# Extract weekday names\nTOP1['weekday'] = TOP1['date'].dt.day_name()\n\n# Count rows for each weekday\nweekday_counts = TOP1['weekday'].value_counts().reset_index()\nweekday_counts.columns = ['weekday', 'count']\n\n# Calculate total number of rows\ntotal_rows = len(TOP1)\n\n# Calculate percentage of rows in weekend (Saturday and Sunday)\nweekend_rows = weekday_counts[weekday_counts['weekday'].isin(['Saturday', 'Sunday'])]['count'].sum()\npercent_weekend = (weekend_rows / total_rows) * 100\n\n# Calculate percentage of rows in weekdays (Monday to Friday)\nweekday_rows = weekday_counts[weekday_counts['weekday'].isin(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'])]['count'].sum()\npercent_weekday = (weekday_rows / total_rows) * 100\n\nprint(f\"Percentage of rows in weekend: {percent_weekend:.2f}%\")\nprint(f\"Percentage of rows in weekdays: {percent_weekday:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:55:19.897674Z","iopub.execute_input":"2024-07-03T05:55:19.898169Z","iopub.status.idle":"2024-07-03T05:55:19.943057Z","shell.execute_reply.started":"2024-07-03T05:55:19.898119Z","shell.execute_reply":"2024-07-03T05:55:19.941598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.3: Distribution of Counts by Weekdays","metadata":{}},{"cell_type":"code","source":"# Assuming TOP1 is already loaded with the data\n# Convert year, month, day columns to datetime format\nTOP1['date'] = pd.to_datetime(TOP1[['year', 'month', 'day']])\n\n# Extract weekday names\nTOP1['weekday'] = TOP1['date'].dt.day_name()\n\n# Count rows for each weekday\nweekday_counts = TOP1['weekday'].value_counts().reset_index()\nweekday_counts.columns = ['weekday', 'count']\n\n# Sort weekdays in chronological order\nweekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nweekday_counts['weekday'] = pd.Categorical(weekday_counts['weekday'], categories=weekdays, ordered=True)\nweekday_counts = weekday_counts.sort_values('weekday')\n\n# Plotting with plotly express\nfig = px.bar(weekday_counts, x='weekday', y='count', \n             color_discrete_sequence=['skyblue'],  # Set color to a single color\n             labels={'count': 'Count of Records', 'weekday': 'Weekday'},\n             title='Distribution of Records by Weekday',\n             text='count')  # Add text labels based on the 'count' column\n\nfig.update_traces(texttemplate='%{text:.2s}', textposition='outside')  # Format text labels\n\nfig.update_layout(xaxis={'categoryorder':'array', 'categoryarray': weekdays},\n                  showlegend=False)  # Hide legend\n\nfig.show(renderer='iframe')","metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:56:01.062026Z","iopub.execute_input":"2024-07-03T05:56:01.062464Z","iopub.status.idle":"2024-07-03T05:56:01.213835Z","shell.execute_reply.started":"2024-07-03T05:56:01.062428Z","shell.execute_reply":"2024-07-03T05:56:01.212492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.4: Distribution of Counts by Weather Condition","metadata":{}},{"cell_type":"code","source":"!pip install meteostat","metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:56:14.591382Z","iopub.execute_input":"2024-07-03T05:56:14.591809Z","iopub.status.idle":"2024-07-03T05:56:30.096043Z","shell.execute_reply.started":"2024-07-03T05:56:14.591774Z","shell.execute_reply":"2024-07-03T05:56:30.094532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\nfrom meteostat import Point, Daily, units\nimport pandas as pd\n\n# Assuming TOP1 is already loaded with the data\n# Create Point for New York City\nnyc = Point(40.7128, -74.0060)\n\n# Convert year, month, day columns to datetime format\nTOP1['date'] = pd.to_datetime(TOP1[['year', 'month', 'day']])\n\n# Define a function to fetch weather condition for a given date\ndef fetch_weather(date):\n    start = date\n    end = date\n    data = Daily(nyc, start, end)\n    data = data.convert(units.imperial)\n    data = data.fetch()\n    \n    if not data.empty:\n        # Assuming we infer conditions like so\n        # Modify this part based on the actual available data and your inference rules\n        if data.iloc[0]['prcp'] > 0:\n            return 'Rainy'\n        else:\n            return 'Sunny'\n    else:\n        return None\n\n# Fetch weather conditions for each date in TOP1 and create a new column 'weather_condition'\nTOP1['weather_condition'] = TOP1['date'].apply(fetch_weather)\n\n# Remove rows where weather condition is None (no data available)\nTOP1 = TOP1.dropna(subset=['weather_condition'])\n\n# Display the DataFrame with the new column\nprint(TOP1)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:56:37.619005Z","iopub.execute_input":"2024-07-03T05:56:37.619477Z","iopub.status.idle":"2024-07-03T05:57:04.449652Z","shell.execute_reply.started":"2024-07-03T05:56:37.61943Z","shell.execute_reply":"2024-07-03T05:57:04.448447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install meteostat plotly","metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:57:35.51532Z","iopub.execute_input":"2024-07-03T05:57:35.515992Z","iopub.status.idle":"2024-07-03T05:57:51.029164Z","shell.execute_reply.started":"2024-07-03T05:57:35.515865Z","shell.execute_reply":"2024-07-03T05:57:51.027331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\nfrom meteostat import Point, Daily, units\n\n# Assuming TOP1 is already loaded with the data\n# Create Point for New York City\nnyc = Point(40.7128, -74.0060)\n\n# Convert year, month, day columns to datetime format\nTOP1['date'] = pd.to_datetime(TOP1[['year', 'month', 'day']])\n\n# Define a function to fetch weather condition for a given date\ndef fetch_weather(date):\n    start = date\n    end = date\n    data = Daily(nyc, start, end)\n    data = data.convert(units.imperial)\n    data = data.fetch()\n    \n    if not data.empty:\n        if data.iloc[0]['prcp'] > 0:\n            return 'Rainy'\n        else:\n            return 'Sunny'\n    else:\n        return None\n\n# Fetch weather conditions for each date in TOP1 and create a new column 'weather_condition'\nTOP1['weather_condition'] = TOP1['date'].apply(fetch_weather)\n\n# Remove rows where weather condition is None (no data available)\nTOP1 = TOP1.dropna(subset=['weather_condition'])\n\n# Display the DataFrame with the new column\nprint(TOP1.head())","metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:57:53.323135Z","iopub.execute_input":"2024-07-03T05:57:53.323714Z","iopub.status.idle":"2024-07-03T05:58:18.372347Z","shell.execute_reply.started":"2024-07-03T05:57:53.32367Z","shell.execute_reply":"2024-07-03T05:58:18.371027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TOP1","metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:58:20.241682Z","iopub.execute_input":"2024-07-03T05:58:20.242117Z","iopub.status.idle":"2024-07-03T05:58:20.265542Z","shell.execute_reply.started":"2024-07-03T05:58:20.242081Z","shell.execute_reply":"2024-07-03T05:58:20.2639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\n\n# Plotting the distribution of TOP1 based on weather conditions\nfig = px.histogram(TOP1, x='weather_condition', \n                   labels={'weather_condition': 'Weather Condition', 'count': 'Number of Records'},\n                   title='Distribution of Records by Weather Condition')\n\n# Add count labels on bars\nfig.update_traces(texttemplate='%{y}', textposition='outside')\n\n# Show the plot\nfig.show(renderer='iframe')","metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:58:24.373791Z","iopub.execute_input":"2024-07-03T05:58:24.374343Z","iopub.status.idle":"2024-07-03T05:58:24.499973Z","shell.execute_reply.started":"2024-07-03T05:58:24.374299Z","shell.execute_reply":"2024-07-03T05:58:24.498647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the counts of each weather condition\nweather_counts = TOP1['weather_condition'].value_counts()\n\n# Calculate the total number of rows\ntotal_rows = len(TOP1)\n\n# Calculate percentages\nweather_percentages = (weather_counts / total_rows) * 100\n\n# Display the percentages\nprint(weather_percentages)\n\n# Alternatively, you can print them in a more readable format\nfor condition, percentage in weather_percentages.items():\n    print(f\"{condition}: {percentage:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:58:34.851908Z","iopub.execute_input":"2024-07-03T05:58:34.852477Z","iopub.status.idle":"2024-07-03T05:58:34.870572Z","shell.execute_reply.started":"2024-07-03T05:58:34.852366Z","shell.execute_reply":"2024-07-03T05:58:34.868202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TOP1.to_csv('TOP1_output.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.5: Calculate Correlations","metadata":{}},{"cell_type":"code","source":"# Assuming your DataFrame is named TOP1\n# Create a mapping of weekdays to numbers\nweekday_mapping = {\n    'Monday': 0,\n    'Tuesday': 1,\n    'Wednesday': 2,\n    'Thursday': 3,\n    'Friday': 4,\n    'Saturday': 5,\n    'Sunday': 6\n}\n\n# Add a new column to the DataFrame for the encoded weekdays\nTOP1['weekday_encoded'] = TOP1['weekday'].map(weekday_mapping)\n\n# Calculate the correlation between count and weekday_encoded\ncorrelation = TOP1['count'].corr(TOP1['weekday_encoded'])\n\nprint(\"Correlation between count and weekday:\", correlation)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:58:57.520203Z","iopub.execute_input":"2024-07-03T05:58:57.520619Z","iopub.status.idle":"2024-07-03T05:58:57.535798Z","shell.execute_reply.started":"2024-07-03T05:58:57.520584Z","shell.execute_reply":"2024-07-03T05:58:57.534517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_hour_count = TOP1['count'].corr(TOP1['hour'])\n\nprint(\"Correlation between count and hour:\", correlation_hour_count)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:59:08.060625Z","iopub.execute_input":"2024-07-03T05:59:08.061051Z","iopub.status.idle":"2024-07-03T05:59:08.068085Z","shell.execute_reply.started":"2024-07-03T05:59:08.061019Z","shell.execute_reply":"2024-07-03T05:59:08.066985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Assuming TOP1 is already loaded with the data\n# Create Point for New York City\nnyc = Point(40.7128, -74.0060)\n\n# Convert year, month, day columns to datetime format\nTOP1['date'] = pd.to_datetime(TOP1[['year', 'month', 'day']])\n\n# Define a function to fetch weather condition for a given date\ndef fetch_weather(date):\n    start = date\n    end = date\n    data = Daily(nyc, start, end)\n    data = data.convert(units.imperial)\n    data = data.fetch()\n    \n    if not data.empty:\n        if data.iloc[0]['prcp'] > 0:\n            return 1  # Rainy\n        else:\n            return 0  # Sunny\n    else:\n        return None\n\n# Fetch weather conditions for each date in TOP1 and create a new column 'weather'\nTOP1['weather'] = TOP1['date'].apply(fetch_weather)\n\n# Remove rows where weather condition is None (no data available)\nTOP1 = TOP1.dropna(subset=['weather'])\n\n# Calculate trip counts per day\ndaily_trip_counts = TOP1.groupby('date').size().reset_index(name='trip_count')\n\n# Merge the weather data with the trip counts\ndaily_trip_counts = daily_trip_counts.merge(TOP1[['date', 'weather']].drop_duplicates(), on='date', how='left')\n\n# Calculate the correlation between trip_count and weather\ncorrelation = daily_trip_counts['trip_count'].corr(daily_trip_counts['weather'])\n\nprint(\"Correlation between trip count and weather (1 for rainy, 0 for sunny):\", correlation)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T06:22:14.352303Z","iopub.execute_input":"2024-07-03T06:22:14.352702Z","iopub.status.idle":"2024-07-03T06:22:38.472524Z","shell.execute_reply.started":"2024-07-03T06:22:14.352672Z","shell.execute_reply":"2024-07-03T06:22:38.4712Z"},"trusted":true},"execution_count":null,"outputs":[]}]}